<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深入浅出索引（上）]]></title>
    <url>%2Fposts%2F2626f727.html</url>
    <content type="text"><![CDATA[我们常常给某些字段加上索引，便于加快查询速度。那么究竟什么是索引呢？？他是怎么工作的呢？？ 数据库的索引比较多，简答来说，索引的出现就是为了提高数据库查询的效率，就像给书增加一个目录一样。没有目录，我们需要一页一页的翻看这本书寻找，但是当我们具有了目录的时候，我们就可以先去查找目录，很快定位到我们所找内容的位置，再直接去访问。 索引的常见模型 索引的出现就是为了提高查询效率，实现索引的方式有很多种，这里我们引入索引模型的概念。可以用于索引的数据结构很多，下面这三种是比较常见的、也比较简单的数据结构，哈希表、有序数组、搜索树。 ¶哈希表 哈希表是一种以键值（key-value）存储数据的结构，我们输入要查询的值key，就可以找到与其对应的值value。它是通过哈希算法将key通过哈希函数生成一个确定的位置然后将该value放在数组的这个位置。 但是，不可避免的，一定会存在多个key经过计算后生成同一个值的情况，我们把这个叫做哈希冲突，解决哈希冲突的方法有很多，这里使用的是拉链法。 假设，现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字，这时候对应的哈希索引的示意图如下所示： 图中，我们可以看出，User2和User4根据身份证号算出来的地址都是N，这时候我们在该地址拉出来一个顺序链表，后面顺次接上属于该位置的Value。我们如果要去查询ID_card_n2对应的名字 将ID_card_n2通过哈希函数算出N； 按顺序遍历链表，找到User2. 这里我们会发现，这里的ID_car_n并不是递增的，这样我们再有新的User时，我们只需要在后面追加，插入效率很高。但是同样，哈希索引做区间查询的速度就会很慢。每次查询只能全表扫描。 因此，哈希表这种结构比较适用于只有等值查询的场景，比如Memcached及其他一些NoSQL引擎。 ¶有序数组 由于上面适用场景有限，这里的有序数组在等值查询和范围查询场景中的性能就都非常的优秀。身份证号有序数组的实现示意图如下： 假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。我们去查ID_card_n2对应的名字，这里因为有序，我们可以去使用二分法快速缩小范围，时间复杂度O(log(n)). 仅仅只看查询效率的话，有序数组的查询效率就是最高的了，但是对于更新数据的时候，我们向中间插入一个数据的时候就必须要将后面的元素全部向后移，这就太消耗资源了！！ 因此，有序数组索引只适用于静态引擎，很少修改的或者根本不修改的数据。 ¶二叉搜索树 二叉搜索树的实现的话，示意图如下： 二叉搜索树的特点： 每个节点的左儿子小于父节点，父节点又小于右节点。这样如果你要查ID_card_n2的话，按照图中的搜索顺序就是按照UserA-&gt;UserC-&gt;UserF-&gt;User2,时间复杂度O(log(n)). 树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。 你可以想象一下一棵 100 万节点的平衡二叉树，树高 20。一次查询可能需要访问 20 个数据块。在机械硬盘时代，从磁盘随机读一个数据块需要 10 ms 左右的寻址时间。也就是说，对于一个 100 万行的表，如果使用二叉树来存储，单独访问一个行可能需要 20 个 10 ms 的时间，这个查询可真够慢的。 为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。 以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。 N 叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。 不管是哈希还是有序数组，或者 N 叉树，它们都是不断迭代、不断优化的产物或者解决方案。数据库技术发展到今天，跳表、LSM 树等数据结构也被用于引擎设计中，这里我就不再一一展开了。 你心里要有个概念，数据库底层存储的核心就是基于这些数据模型的。每碰到一个新数据库，我们需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景。 InnoDB的索引模型 在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表陈伟索引组织表。InnoDB使用了B+树索引模型，因此数据都是存储在B+树中的。 每一个索引在InnoDB中对应一颗B+树。 假设，我们有一个主键列为ID的表，表中有字段k，并且在k上有索引。 这个表的建表语句是： 123456mysql&gt; create table T( id int primary key, k int not null, name varchar(16), index (k))engine=InnoDB;&#125; 表中R1~R5的(ID,k)值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树的示例示意图如下: 从图中两棵树的对比，我们可以清楚的看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引。 主键索引的叶子节点存储的是整行数据。在InnoDB中，主键索引也被称为聚簇索引（clustered index） 非主键索引的叶子节点内容是主键的值。在InnoDB中，非主键索引也被称为二级索引(secondary index) 若该表没有主键，只有一个普通索引，那么在普通索引的B+树中存储的是由MySQL生成的一个rowId作为主键。 ¶❓基于主键索引和普通索引的查询有什么区别? select * from T where ID = 500,主键查询方式，只需要去搜索ID这颗B+树 select * from T where k = 5,普通索引查询方式，需要先搜索k索引树，得到ID的值为500，再到ID索引树搜索一次。这个过程称为回表。 因此，我们在多个索引均可查询的情况下，我们要先优先使用主键索引。 ¶索引维护 B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。 如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。 基于上面的索引维护过程说明，我们来讨论一个案例： 你可能在一些建表规范里面见到过类似的描述，要求建表语句里一定要有自增主键。当然事无绝对，我们来分析一下哪些场景下应该使用自增主键，而哪些场景下不应该。 自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。 插入新记录的时候可以不指定 ID 的值，系统会获取当前 ID 最大值加 1 作为下一条记录的 ID 值。 也就是说，自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。 而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。 除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？ 由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整型（bigint）则是 8 个字节。 显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。 有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的： 只有一个索引； 该索引必须是唯一索引。 你一定看出来了，这就是典型的 KV 场景。由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事务隔离：我什么时候能看到你的改动？]]></title>
    <url>%2Fposts%2F67741b0f.html</url>
    <content type="text"><![CDATA[事务，学过数据库的应该都至少知道它的概念，我们总是会用到事务。最典型的一个案例就是： 小王（100元）转给小明（0元）100。转账有下面一系列场景： 先查询余额； 然后在转出方账户减少100元； 之后在转入方账户增加100元； 最后一步，再查询一次。 这四步完成一个转账流程。那么我们假设这时候在第二步之后出现问题了。这时候小王的100元已经扣掉了，小明这边还没有增加。崩掉之后，我们再去查询，两边都是0元，小王说我转出去了，小明说我没收到，这不就乱了套了！！！ 这时候，事务的重要性就出现了，事务就是保证一整套操作中的分操作，要么全部成功，要么全部失。在MySQL中，事务支持是在引擎层中实现的。MySQL是一个支持多引擎的系统，但那不是所有的引擎都会支持事务，MySQL原生的MyISAM就不支持事务，这也是InnoDB取代MyISAM的原因之一。 隔离性与隔离级别 事务最应该先提到的是四大性质ACID：原子性、一致性、隔离性、持久性。 隔离性体现在哪里呢？？ 一般在一个系统中，会有多个事务同时操作数据库，就可能会出现脏读、不可重复读、幻读的问题，为了解决这些问题，就出现了“隔离级别”这个概念。 在谈隔离级别之前。我们先要了解一个常识，你越隔离的程度越高，效率就会越低。因此在很多时候我们都要在二者之间寻找一个平衡点。SQL标准的事务隔离级别包括： 读未提交 一个事务还没提交时，他做的变更就能被别的事务看到 读提交 一个事务提交之后，他做的更改才能被其他事务看到 可重复读 一个事务执行过程中看到的数据，总是跟这个事务在启动是看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化 顾名思义是对同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 ¶解释 12mysql&gt; create table T(c int) engine=InnoDB;insert into T(c) values(1); 那么在不同的隔离级别下，事务A的返回值都是多少呢？？ 读未提交：V1的值就是2.这时候事务B虽然还没有提交，但是结果已经被A看到了。因此，V2、V3也都是2。 读提交：V1的值是1，V2是2.事务B的更新在提交后才能被A看到。所以，V3也是2。 可重复读：V1、V2是1，V3是2.之所以V2还是1，遵循的就是：事务在执行期间看到的数据前后必须一致。 串行化：每次执行读写操作时，会有一个锁将当前进程锁住，所以，当事务B在执行1-2的时候，会被锁住，直到事务A提交后，事务B才能继续执行。所以从A的角度看的话，V1、V2的值是1，V3的值是2。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在启动时创建的，整个事务存在其在都用这个视图。在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是，“读提交”隔离级别下直接返回记录上的最新值，没有视图概念；而”串行化“隔离级别下直接用加锁的方式来避免并行访问。 MySQL的隔离配置方式是，将启动参数transaction-isolation的值设置成READ-COMMITTED.可以使用show variables来查看当前的值。 1mysql&gt; show variables like 'transaction_isolation'; 演示截图： 每个隔离的方式都有它存在的道理。根据自己业务的实际场景而决定。我们来看看”可重复读“的场景，看一个数据校对逻辑。 假设你在管理一个个人银行账户表。一个表存了每个月月底的余额，一个表存了账单明细。这时候你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。 这时候我们使用”可重复读“就比较方便，事务启动时的视图可以认为是静态的，不受其他事务更新的影响。 事务隔离的实现 上面介绍了隔离级别，我们再来看看事务隔离的具体实现。这里我们说”可重复读“。 在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。 当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。 同时你会发现，即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的。 你一定会问，回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。 什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。 基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。我见过数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库。 除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库，这个我们会在后面讲锁的时候展开。 事务的启动方式 如前面所述，长事务有这些潜在风险，我当然是建议你尽量避免。其实很多时候业务开发同学并不是有意使用长事务，通常是由于误用所致。MySQL 的事务启动方式有以下几种： 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。 set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。 有些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。 因此，我会建议你总是使用 set autocommit=1, 通过显式语句的方式来启动事务。 但是有的开发同学会纠结“多一次交互”的问题。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。如果你也有这个顾虑，我建议你使用 commit work and chain 语法。 在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。 你可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。 1select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))&gt;60;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一条SQL更新语句]]></title>
    <url>%2Fposts%2Fd37e0955.html</url>
    <content type="text"><![CDATA[UPDATE引发的一系列 ¶引言 通过上一篇文章我们知道，一条SQL查询语句的执行流程：连接器—&gt;分析器—&gt;优化器—&gt;执行器等功能模块顺次执行的，最后到达存储引擎进行搜寻。 那么，一条UPDATE又是怎么样的呐？？我们来看看！ 我们先来说说经常听到的MySQL可以恢复到半个月内的任意一秒的状态，相信当我们听到这里的时候都比较惊叹，怎么会如此强大？？那么MySQL是怎么做到的呢？？ 这就得从我们的一个表的更新语句说起，下面这是一个表的创建语句，他有一个主键ID，和一个整形字段’C’: 1mysql&gt; create table T(ID int primary key,c int); 若要将ID = 2 这一行的值增加1，SQL语句就是如下这样： 1mysql&gt; update T set c = c + 1 where ID = 2; 根据上一张的SQL执行语句执行顺序的那张图，更新语句也是一样的流程，我们再拿过来看看： 你要去操作数据库，第一步肯定是要先去连接相应的数据库，这是连接器的工作。 在查询中，记得提过一个缓存机制，但是在一张表被更新的的时候，缓存就会全部失效。因此，这句更新操作语句会将表T上的缓存结果全部清空。这也就是我们一般不推荐使用缓存机制的原因。 接下来，分析器来分析这句SQL语句，通过词法和语法解析，知道这是一条UPDATE语句。优化器决定要使用ID这个索引。然后执行器负责具体的执行，进行索引遍历，找到ID=2的这一行，然后进行更新。 当然，他也具有与查询语句大不同的地方，更新流程还涉及两个重要的日志模块：redo log(重做日志)和binlog(归档日志)。这两个日志在设计思路上非常有趣，可以去深究一下，用在自己的程序中。 ¶重要的日志模块 ---- redo log 近代，很多酒店掌柜都有一个黑板，专门来记录客人的赊账情况。赊账的人少的话，他只用直接将赊账的人的名字写在黑板上，但是如果赊账的人多了，那么就需要一个记账本来专门记录了。 这样的话，掌柜的一般有两种做法可以选择： 直接在账本上进行操作，每次在账本上添加或者删除。 现在黑板上记下这次的账，等到打烊了，再将今日的账总到赊账本上。 一般精明的生意人自然会去选择第二种方法，前者对于账本的操作太频繁，并且账本本来就是一个大数据大容量的物品，每次的查找修改耗费太多的精力。 相比之下，老板会选择在黑板上先记下当前赊账的人的名字和金额，之后再记在账本上比较高效！ 同样的，在MySQL中也会具有这样的问题，每一次的更新操作都要写入磁盘，然后磁盘也要进行搜索，找到那条数据进行写入更新。IO本来就是一个高消耗，高成本的操作。为了解决此类问题，MySQL的设计者就使用了一块“小黑板”来提升更新效率。 黑板和账本完美的配合，类比到MySQL就是一个WAL技术（Write-Ahead Logging），他的关键就是先写日志，之后再写入磁盘。 具体一点来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写道redo log（黑板）里面，并更新内存，这时候对外显示的就是当前的更新已经完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘中去，而这个操作往往是在系统比较空闲的时候去进行的。 当赊账(redo log)不多的时候，老板完全可以等到打烊(空闲)的时候进行写入操作，但是redo log满了呢？？那么老板只能放下手中的活，将之前的一部分写入到磁盘中，从redo log中擦掉这些记录，腾出新的空间。 InnoDB中的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小事1GB，那么这个redo log就可以总共记录4GB的操作。而它的设计者，更加完美的考虑，将他设计成了环形，这样就可以不断循环写入，就像下图这样： write pos 就是当前记录的位置，一边写一边后移，写道第3号文件末尾就回到0号文件的开头。chechpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos 和 checkpoint中间的位置就是当前还空闲的位置，如果write pos追上了checkpoint，，就表明这redo log被写满了，先去进行写入数据文件操作，然后将checkpoint向前推移，之后再继续写。 有了redo log ，InnoDB就可以保证及时数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 crach-safe也就是就算数据库异常重启，他仍然处于之前的状态，不会进行变化，可以继续运行。 ¶重要的日志模块 ---- binlog MySQL总体来分，有两个架构层：一个是Server层，主要负责功能层面的事情；另一个是存储引擎层，负责存储相关。redo log属于InnoDB引擎特有的日志。，而Server层也有属于自己的日志，称为binlog(归档日志). 为什么要存在两份日志呢？？ 因为最开始并没有InnoDB，只有一个自带的引擎MyISAM,但是MyISAM并没有crash-safe能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件的形式引入MySQL的。他们就使用InnoDB去实现了crash-safe能力，也就是InnoDB的日志系统------redo-log实现了。 这两种日志的不同点： redo log是InnoDB特有的引擎；binlog是MySQL的Server层实现的，所有引擎都可以使用。 redo log是物理日志，记录的是“在某个数据上进行了什么修改”；binglog是逻辑日志，记录的是这个语句的原始逻辑：“给ID=2这一行的c字段加上1”. redo log是循环写的。空间是固定的，会使用完。而binlog是可以追加写入的。当达到一定大小后，会切换到下一个，并且不会覆盖前一个。 了解完这两个日志的概念之后，我们去看看执行器和InnoDB是如何执行这个简单的update的内部流程。 执行器先找引擎拿到ID=2的这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就存在于内存中，就直接返回给执行器，否则，先去磁盘中读取放入内存，之后再返回。 执行器拿到引擎给的行数据之后，把这个值加1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行数据更新到内存中，同时将这个更新操作记录到redo log中，此时redo log处于prepare状态。然后告知执行器执行完成了，可以随时提交事务。 执行器生成这个操作的binlog，并把binlog写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。 这个update的执行流程如下图所示，浅色框表示在InnoDB内部执行的，深色表示是在执行器中执行的。 最后三步看上去优点绕，将 redo log 的写入拆成了两个步骤：prepare和cpmmit，这就是两阶段提交 ¶两阶段提交 为什么要放着简单的一次提交不用，而非得去用什么两阶段提交呢？这是为了让两份日志之间的逻辑一致。这就涉及到了本文开头中所提到的问题：怎么让数据库恢复到半个月内任意一秒的状态呢？ 如果半个月之内的可以恢复，那么备份系统中一定保存了最近半个月的所有binlog，同时系统和会定期做整库备份。这里的定期，取决于数据库的重要性。 当需要恢复到指定时间时候的数据库时，我们可以这样做： 首先，找到最近一次全量辈份，若果运气好，可能就是昨天晚上刚备份的，直接恢复到临时库 然后，从备份时间开始，将辈份的binlog依次取出，重新放到从备份时间到误删表的时刻。 这时候就恢复到和误删时候一模一样了，然后从临时库中取出来，恢复到线上库去。 话说回来，我们为什么要两阶段提交，我们试试反证法： redo log和binlog是两个独立的逻辑，如果不用两阶段提交，要么就是先写完redo log再写binlog，要么就是相反。 假设c = 0，做c+1操作的update语句在写完第一个日志后，第二个日志还没有写完期间发生了crash，会出现什么情况呢？？ 先写redo log 后写 binlog 假设在redo log写完，binlog还没写完的时候，MySQL进程异常重启。回复过来后根据redeo log进行重写恢复，c仍为1。 但是因为binlog没写完就crash了，这时候binlog没有记录这个语句，因此，之后备份日志的时候，存起来的binlog就会少了这一条逻辑语句。那么后面用binlog去恢复临时库的话，就没有这条语句，则c = 0. 先写binlog 后写redo log 如果在binlog写完之后crash，由于redo log还没有写入，崩溃之后，该事务无效，因此等于0。但在恢复库的时候，直接去读binglog中的数据，读出来时已经更改为1，与源数据库不符。 这个不只是应用在误删库后的恢复上，而且还会应用在主从数据库备份，复制上。保证了主从数据库的一致性。 为了保证redo log的crash-safe能力，建议将innodb_flush_log_at_trx_commit设置为1，这样每次事务的redo log都直接持久化到磁盘。 binlog中的sync_binlog设置成1，保证每次事务的binlog持久化到磁盘]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一条SQL查询语句]]></title>
    <url>%2Fposts%2F4a263821.html</url>
    <content type="text"><![CDATA[开始将MySQL进行体系学习，学习一个东西直接去了解它的内部或者底层，过于高深，咱搞不来，我们就从一个简单的SQL语句看起 1mysql&gt;select * from user where id = 1; 这是一个简单的查询语句，我们输入这句话之后，按下Enter就会给我们一个返回结果。那他是怎么做到的输入这样一句话就可以给我们查到我们想要的数据的呢？？ 我们去一探究竟~ ~ ~ 我们先看看MySQL的基本架构，它是由哪些功能模块组成的呢？？ MySQL的基本架构组成 SQL执行过程我用一张图片来表示一下： 这张图片很清楚的表现了一个SQL执行过程的逻辑流程， 大体来说，MySQL可以分为Server层和引擎层两部分。 Server层 连接器 查询缓存 分析器 执行器等 这一层涵盖可MySQL大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 存储引擎层 负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Mymory等多个存储引擎。现在最常用的就是InnoDB引擎，他从MySQL5.5.5开始成为了默认的存储引擎。 当然，你可以不使用默认的InnoDB，在创建一个Table的时候我们可以通过engine=Memory这样的语句去指定引擎创建表，不同存储引擎的表数据存取方式不同，支持的功能也不同。 我们开始对Server层的组件进行介绍学习 ¶连接器 我们要相对一个数据库进行操作，首先需要进行连接到该数据库上。连接是我们连接器去做的事情。负责跟客户端建立连接、获取权限、维持和管理连接。 连接命令： 1mysql -h ip -P port -u root -p 接下来会提示你输入密码，当然，你的密码也可以像用户名一样跟在-p后面，但是因为明文容易出现密码泄露等问题，强烈不建议明文输入密码。 完成与MySQL的TCP握手之后，开始验证身份，无外乎出现以下两种情况： 用户名或密码不对，返回错误 1ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES) 然后客户端结束执行. 用户名密码认证通过，连接器回到一张用户权限表中查询当前用户所具有的权限，之后的操作都会根据这个权限去先进行判断。 如果你用管理员登陆之后，对已经连接着的用户进行权限更改，在该用户重新创建新连接之前，权限值并没有进行更新，仍为修改前的权限。 我们可以通过下面的命令进行查询当前建立连接的用户以及状态： 1mysql&gt; show processlist; 可以看到，这里有两个连接，有一个连接什么操作都没有，所以他的状态栏就写的sleep空闲连接。默认自动中断时间是8小时，是由wait_timeout控制的。 ¶长连接和短连接 长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。 短连接事直每次执行完很少的几次查询之后就断开，下次查询会重新再连接一个。 长短连接的利弊 数据库的连接需要去调用相应的JDBC驱动去进行连接，比较耗费资源，所以我们尽量的去减少数据库连接操作，也就是尽量的去使用长连接。 但是问题又来了，一直使用一个长连接的话，你会发现你的MySQL占用内存涨的非常快，这是因为只有断开连接的时候，才会继续进行一次资源的释放。所以如果长连积累下来，可能导致内存占用太大，强行被系统kill掉！ 当然，解决方案来了 定期断开长连接。使用一段时间，或者进行了占用内存较大的查询后，就断开连接进行一次重新连接。 你若使用的是MySQL5.7之后的版本，可以在每次执行完一个大操作之后进行mysql_reset_connection来重新初始化连接资源，这个过程是不需要重连和做权限验证的，会将连接恢复到刚刚创建完时的状态。 ¶查询缓存 连接建立完成后我们就来执行SELECT语句，首先并不会直接去存储引擎里面去检索，而是会先在缓存中进行查询。 只要是之前经过查询并且开启缓存机制，所拆卸难过的语句和结果会以key-value的的形式存在于缓存内存中。若你的SQL语句在缓存中的key有相同的值，那么会击中缓存，直接进行value的返回。 不推荐使用缓存机制 缓存的失效非常频繁，只要对任何一个表进行更新，那么该表上的所有缓存将被清空。也就是说，可能耗费了很大的资源把缓存进行存储了，但是随即而来的一个更新操作，可能让之前费了很大劲进行存储的缓存失效。 因此，基于击中率极低的缓存，我们尽量不在频繁更新的表中使用，若是存在一些不经常更新的表或者一些静态表（系统配置表），这一类表我们比较适合使用缓存机制。 然而，MySQL给我们准备了非常人性化的方式。我们可以将参数**“query_cache_type&quot;设置成&quot;DEMAND”,这样对于默认的SQL语句都不会查询缓存。而对于我们确定要去查询缓存的语句，我们可以去使用&quot;SQL_CACHE&quot;**去显式指定。 1mysql &gt; select SQL_CACHE * from user where id = 10; 注：在MySQL8.0之后，MySQL删除了缓存功能！！ ¶分析器 缓存中没有，我们就要去存储引擎中去进行查询，当然在这之前，我们要对你输入的语句进行分析，要知道你在查什么？？那么就需要在分析器中进行SQL语句分析。 词法分析 你输入的语句是由多个字符串和空格组成的一条SQL语句，例如下面这条： 1mysql &gt; select * from user where id = 1; 首先，他检测到米输入的第一个单词是select，这时候识别出了这是一个查询语句； 第二步，检测到你要查询的数据是“*”，也就是一整条记录的所有信息； 第三步，识别出表名user，知道你所要查询的信息在哪个表中； 最后，将条件id = 1，识别成有一列叫id他的值要是1。 语法分析 做完词法分析之后，我们就要去进行语法分析，语法分析器根据语法规则，判断用户输入的SQL语句是不是满足MySQL的语法。我们试一个错误的语法： 1mysql &gt; select * fromg user where user_id = "2017211005"; 我们可以看到报错 1ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'elect * from user where user_id="2017211005"' at line 1 这里的错误提示，我们看到user near ，一般提示的错误在语法第一次出现错误的地方，我们需要去关注。 ¶优化器 过了分析器，分析了你的SQL语句，知道了你要做什么。但在开始执行之前，他还需要经过优化器的处理。 优化器的基本作用 当你的表中具有多个索引的时候，优化器根据你的SQL选择你要去使用哪个索引。 或者当你具有多表关联，可以去选择更好的连接顺序 比如下面的SQL语句 1mysql&gt; select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20; 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。 这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。 优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。 ¶执行器 分析器知道了你要做什么，优化器知道了该怎么去做，现在就要去执行器执行语句了。 开始执行的时候，就要先去判断你对这个表有没有操作权限，若没有权限就会出现下面的情形。 123mysql&gt; select * from T where ID=10;ERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T' 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的： 1.调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 2.调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 3.执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 至此，这个语句就执行完成了。 对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。 你会在数据库的慢查询日志中看到一个 “rows_examined” 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。 本章提问： 如果表 T 中没有字段 k，而你执行了这个语句 select * from T where k=1, 那肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是在我们上面提到的哪个阶段报出来的呢？ 答案： 分析器。一方面受到Oracle设计思想的影响，另外在《高性能mysql》里提到解析器和预处理器。解析器处理语法和解析查询, 生成一课对应的解析树。预处理器进一步检查解析树的合法。比如: 数据表和数据列是否存在, 别名是否有歧义等。如果通过则生成新的解析树，再提交给优化器；]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过注解注入Bean]]></title>
    <url>%2Fposts%2Fd81b6328.html</url>
    <content type="text"><![CDATA[通过注解注入Bean ¶背景 我们谈到Spring的时候一定会提到IOC容器、DI依赖注入，Spring通过将一个个类标注为Bean的方法注入到IOC容器中，达到了控制反转的效果。那么我们刚开始接触Bean的时候，一定是使用xml文件，一个一个的注入，就例如下面这样。 1&lt;bean id="bean" class="beandemo.Bean" /&gt; 我们的项目一般很大的话，就需要成千上百个Bean去使用，这样写起来就很繁琐。那么Spring就帮我们实现了一种通过注解来实现注入的方法。只需要在你需要注入的类前面加上相应的注解，Spring就会帮助我们扫描到他们去实现注入。 xml扫描包的方式 1&lt;context:component-scan base-package="com.company.beandemo"/&gt; ¶通过注解注入的一般形式 一般情况下，注入Bean有一个最直白，最易懂的方式去实现注入，下面废话先不多说，先贴代码。 Bean类 12public class MyBean&#123;&#125; Configuration类 123456789//创建一个class配置文件@Configurationpublic class MyConfiguration&#123; //将一个Bean交由Spring进行管理 @Bean public MyBean myBean()&#123; return new MyBean(); &#125;&#125; Test类 与xml有一点不同，这里在Test中，实例化的不再是ClassPathXmlApplicationContext，而是获取的AnnotationConfigApplicationContext实例。 123ApplicationContext context = new AnnotationConfigApplicationContext(MyConfiguration.class);MyBean myBean = cotext.getBean("myBean",MyBean.class);System.out.println("myBean = " + myBean); 上面的代码中MyBean也就是我们需要Spring去管理的一个Bean，他只是一个简单的类。而MyConfiguration中，我们首先用@Configuration注解去标记了该类，这样标明该类是一个Spring的一个配置类，在加载配置的时候会去加载他。 在MyConfiguration中我们可以看到有一个方法返回的是一个MyBean的实例，并且该方法上标注着@Bean的注解，标明这是一个注入Bean的方法，会将下面的返回的Bean注入IOC。 ¶通过构造方法注入Bean 我们在生成一个Bean实例的时候，可以使用Bean的构造方法将Bean实现注入。直接看代码 Bean类 1234567891011121314151617@Componentpublic class MyBeanConstructor &#123; private AnotherBean anotherBeanConstructor; @Autowired public MyBeanConstructor(AnotherBean anotherBeanConstructor)&#123; this.anotherBeanConstructor = anotherBeanConstructor; &#125; @Override public String toString() &#123; return "MyBean&#123;" + "anotherBeanConstructor=" + anotherBeanConstructor + '&#125;'; &#125;&#125; AnotherBean类 123@Component(value="Bean的id，默认为类名小驼峰")public class AnotherBean &#123;&#125; Configuration类 1234@Configuration@ComponentScan("com.company.annotationbean")public class MyConfiguration&#123;&#125; 这里我们可以发现，和一般方式注入的代码不一样了，我们来看看新的注解都是什么意思： @AutoWired 简单粗暴，直接翻译过来的意思就是自动装配🔧，还不理解为什么叫自动装配🔧？看了下一个注解的解释你就知道了。若是在这里注入的时候指定一个Bean的id就要使用@Qualifier注解 @Component（默认单例模式） 什么？？这翻译过来是零件，怎么感觉像是修汽车？？是的，Spring管理Bean的方法就是修汽车的方式。我们在需要将一个类变成一个Bean被Spring可以注入的时候加上注解零件@Conmonent，那么我们就可以在加载Bean的时候把他像零件一样装配🔧到这个IOC汽车上了 在这里我们还有几个其他的注解也可以实现这个功能，也就是细化的@Component： @Controller 标注在Controller层 @Service 标注在Service层 @Repository 标注在dao层 @ComponentScan(&quot;&quot;) 还是翻译，零件扫描，我们去看看括号里的“零件仓库”里面，哪些“零件”（类）需要被装载，Spring就会去扫描这个包，将里面所有标注了@Component的类进行注入。 这里的通过构造方法进行注入就很好理解了，我们在装配MyBean这个零件的时候，突然发现他必须在AnotherBean的基础上才能安装到IOC里面，那么我们就在每次装配MyBean的时候自动装配🔧一个AnotherBean进去。举个🌰吧： 还是以汽车为例，我们在踩油门出发之前，是不是必须发车？？这里的AutoWired的内容就像发车，你不发车，这个油门你踩断都没有用，他都不会走。 ¶通过set方法注入Bean 我们可以在一个属性的set方法中去将Bean实现注入，看代码吧 MyBean类 1234567891011121314151617@Componentpublic class MyBeanSet &#123; private AnotherBean anotherBeanSet; @Autowired public void setAnotherBeanSet(AnotherBean anotherBeanSet) &#123; this.anotherBeanSet = anotherBeanSet; &#125; @Override public String toString() &#123; return "MyBeanSet&#123;" + "anotherBeanSet=" + anotherBeanSet + '&#125;'; &#125;&#125; Configuration类 和 Test类 同上一个，就不贴了 这里我们发现在setter方法上我们有一个@AutoWired,与上面不同的是，我们不会在实例化该类时就自动装配🔧这个对象，而是在显式调用setter的时候去装配。 ¶通过属性去注入Bean 我们前面两种注入的方式诸如时间不同，并且代码较多，若是通过属性，即就是 12345678910111213@Componentpublic class MyBeanProperty &#123; @Autowired private AnotherBean anotherBeanProperty; @Override public String toString() &#123; return "MyBeanProperty&#123;" + "anotherBeanProperty=" + anotherBeanProperty + '&#125;'; &#125;&#125; 这里我们可以看到我们这个类中需要使用AnotherBean这个实例对象，我们可以通过@AutoWired去自动装配它。 对于有些小伙伴问私有属性，Spring怎么去加载它到IOC的？推荐去看看反射 ¶通过List注入Bean MyBeanList类 1234567891011121314@Componentpublic class MyBeanList &#123; private List&lt;String&gt; stringList; @Autowired public void setStringList(List&lt;String&gt; stringList) &#123; this.stringList = stringList; &#125; public List&lt;String&gt; getStringList() &#123; return stringList; &#125;&#125; MyConfiguration类 123456789101112@Configuration@ComponentScan("annoBean.annotationbean")public class MyConfiguration &#123; @Bean public List&lt;String&gt; stringList()&#123; List&lt;String&gt; stringList = new ArrayList&lt;String&gt;(); stringList.add("List-1"); stringList.add("List-2"); return stringList; &#125;&#125; 这里我们将MyBeanList进行了注入，对List中的元素会逐一注入。下面介绍另一种方式注入List MyConfiguration类 123456789101112@Bean //通过该注解设定Bean注入的优先级，不一定连续数字 @Order(34) public String string1()&#123; return "String-1"; &#125; @Bean @Order(14) public String string2()&#123; return "String-2"; &#125; 注入与List中泛型一样的类型，会自动去匹配类型，及时这里没有任何List的感觉，只是String的类型，但他会去通过List的Bean的方式去注入。 第二种方式的优先级高于第一种，当两个都存在的时候，若要强制去使用第一种方式，则要去指定Bean的id即可 ¶通过Map去注入Bean 1234567891011121314@Componentpublic class MyBeanMap &#123; private Map&lt;String,Integer&gt; integerMap; public Map&lt;String, Integer&gt; getIntegerMap() &#123; return integerMap; &#125; @Autowired public void setIntegerMap(Map&lt;String, Integer&gt; integerMap) &#123; this.integerMap = integerMap; &#125;&#125; 1234567891011121314151617@Bean public Map&lt;String,Integer&gt; integerMap()&#123; Map&lt;String,Integer&gt; integerMap = new HashMap&lt;String, Integer&gt;(); integerMap.put("map-1",1); integerMap.put("map-2",2); return integerMap; &#125; @Bean public Integer integer1()&#123; return 1; &#125; @Bean public Integer integer2()&#123; return 2; &#125; 同样这里也具有两种方式去注入Map类型Bean，且第二种的优先值高于第一种 以上就是Bean通过注解注入的几种方式，大家可以对比着xml注入的方式去看。]]></content>
      <categories>
        <category>Java</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于并发的sychronized和@GuardedBy]]></title>
    <url>%2Fposts%2F888f520a.html</url>
    <content type="text"><![CDATA[在Java中，我们一般会使用synchronized给一段在高并发情况下会产生异常的代码加上一个同步锁，实现在运行该代码的时候，多个线程会去先竞争这个同步锁，获得这同步锁的进程会去执行该段程序，其他线程处于wait状态，直到获得该锁的线程运行完毕释放锁后，其他线程会再次进入竞争状态，竞争该锁的使用权…以此类推！ 对于多个线程和一个加锁的程序段的运行流程 线程1、线程2、线程3处于就绪状态 三个线程竞争synchronized同步锁 线程1夺到锁的使用权，线程2、3恢复到就绪状态 线程1拿着同步锁进行读写操作，期间没有任何线程打扰 线程1执行完成后，将同步锁归还（释放） 处于就绪状态的线程2、3进行竞争锁的使用权】 …同样的操作 没有synchronized的后果 若是没有synchronized同步锁，举个简单的例子，一段代码执行需要一定的时间，若是一个线程正在对一个共享数据进行操作中，另一个线程也去访问该共享数据，这时候第一个正在执行的线程没有返回他的执行结果，也因此第二个线程读取的共享数据是源数据，也就是没有更改之前的，这就是所谓的“脏数据”。 ¶Example: 小明将自己的零花钱存在一个银行卡里面（1000元），小明的爸爸不放心，则会不定时的去查小明的账户，小明这天刚拿着爸爸给的100元零花钱去存，小明的老爸也想看看小明把自己刚给他的100元是不是如他所说的存了起来，两个人去了两家不同的自动取款机，当小明把钱放进去正在点钞，还没有入账的时候，小明的父亲输入完密码，数据读取中，因为点钞需要一定的时间，在进入账户之前，小明的爸爸已经查到了只有1000元，气呼呼地准备回家去揍小明，小明这时候刚点完钞，入了账！！！请问：小明这次的被打冤枉不冤枉？？ 放在程序中i就是一个线程进行i++操作，读取–》+1–》写入，而另一个需要使用i的数据，而他在读取的时候，i++正在进行+1操作！！！这时候我们读取的i会丢失1.这种不安全的线程操作！！ 我们应该将该方法实现原子性，是一个原子方法，我们加上synchronized同步锁，就不会出现多个线程同时访问一个共享数据的情况，也就不会出现所谓的“脏数据”了！ ¶使用方法 ¶修饰一个代码块 一个线程访问一个对象中的synchronized(this)同步代码块时，其他试图访问该对象的线程将被阻塞。我们看下面一个例子： ¶Deom1 123456789101112131415161718192021222324 public class SyncThread implements Runnable &#123; private static int count; public SyncThread() &#123; count = 0; &#125; public void run() &#123; synchronized(this) &#123; for (int i = 0; i &lt; 5; i++) &#123; try &#123; System.out.println(Thread.currentThread().getName() + ":" + (count++)); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; public int getCount() &#123; return count; &#125;&#125; Syncthread的调用 12345SyncThread syncThread = new SyncThread();Thread thread1 = new Thread(syncThread, "SyncThread1");Thread thread2 = new Thread(syncThread, "SyncThread2");thread1.start();thread2.start(); Result: 12345678910SyncThread1:0 SyncThread1:1 SyncThread1:2 SyncThread1:3 SyncThread1:4 SyncThread2:5 SyncThread2:6 SyncThread2:7 SyncThread2:8 SyncThread2:9 该代码的运行结果中未出现交互出现Thread1、2的运行结果，也就是两个线程对同一个对象进行操作，然后1不放锁，2运行不了！！！ 我们试一下改一下调用方式： 1234Thread thread1 = new Thread(new SyncThread(), "SyncThread1");Thread thread2 = new Thread(new SyncThread(), "SyncThread2");thread1.start();thread2.start(); 我们就会发现结果截然不同了呢！！！ result： 12345678910SyncThread1:0 SyncThread2:1 SyncThread1:2 SyncThread2:3 SyncThread1:4 SyncThread2:5 SyncThread2:6 SyncThread1:7 SyncThread1:8 SyncThread2:9 发现了什么！！交替执行了！！因为这里的synchronized是给这个SynThread产生的对象加了同步锁，而我们两个线程使用在两个不同的线程上！！！所以这里不会存在竞态关系，故只需要线程抢到ＣＰＵ资源就可以运行了，根据时间片的切分，会不断的切换线程进行执行！！就出现了上面的结果。 ¶Demo2 我们尝试在一个方法中，加入一个含有锁的代码块以及一个不含有锁的代码块，并且该类还有另一个方法（未加锁），被统一对象调用我们看一看结果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * @author :DengSiYuan * @date :2019/3/14 22:51 * @desc : */public class Counter implements Runnable&#123; private int count; public Counter() &#123; count = 0; &#125; public void countAdd() &#123; synchronized(this) &#123; for (int i = 0; i &lt; 5; i ++) &#123; try &#123; System.out.println("有锁的代码块"+Thread.currentThread().getName() + ":" + (count++)); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; //同一个方法中的未加锁代码块 for (int j = 0; j &lt; 5; j ++) &#123; try &#123; System.out.println("没有锁的代码块"+Thread.currentThread().getName() + ":" + (count++)); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; //非synchronized代码块，未对count进行读写操作，所以可以不用synchronized public void printCount() &#123; for (int i = 0; i &lt; 5; i ++) &#123; try &#123; System.out.println(Thread.currentThread().getName() + " count:" + count); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; @Override public void run() &#123; String threadName = Thread.currentThread().getName(); if (threadName.equals("A")) &#123; countAdd(); &#125; else if (threadName.equals("B")) &#123; printCount(); &#125; &#125;&#125; 调用代码： 1234567public static void main(String[] args) &#123; Counter counter = new Counter(); Thread thread1 = new Thread(counter, "A"); Thread thread2 = new Thread(counter, "B"); thread1.start(); thread2.start(); &#125; 执行结果： 123456789101112131415B count:0有锁的代码块A:0有锁的代码块A:1B count:2B count:2有锁的代码块A:2B count:3有锁的代码块A:3B count:4有锁的代码块A:4没有锁的代码块A:5没有锁的代码块A:6没有锁的代码块A:7没有锁的代码块A:8没有锁的代码块A:9 这里我们就可以很清楚的看到以下现象： 同一个方法中，一个代码块加synchronized同步锁，另一个不加，按顺序执行。（当前执行对象的锁被获取了） 不同的方法，一个加锁，另一个未加，则互不影响！ ¶给某个对象加锁 ¶Demo3 上面小明的案例中，我们为了不让小明挨打，我们可以对该账户加锁，只要有用户正在操作该用户，那么我们就可以让其他人不能操作！ 演示代码：(未加锁) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class Account &#123; private String name; private float amount; public Account(String name, float amount) &#123; this.name = name; this.amount = amount; &#125; //存钱 public void deposit(float amt) &#123; amount += amt; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //取钱 public void withdraw(float amt) &#123; amount -= amt; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public float getBalance() &#123; return amount; &#125;&#125;/** * 账户操作类 */class AccountOperator implements Runnable&#123; private Account account; public AccountOperator(Account account) &#123; this.account = account; &#125; @Override public void run() &#123; //synchronized (account) &#123; account.deposit(500); account.withdraw(500); System.out.println(Thread.currentThread().getName() + ":" + account.getBalance()); //&#125; &#125; public static void main(String[] args) &#123; Account account = new Account("小明", 10000.0f); AccountOperator accountOperator = new AccountOperator(account); final int THREAD_NUM = 5; Thread threads[] = new Thread[THREAD_NUM]; for (int i = 0; i &lt; THREAD_NUM; i ++) &#123; threads[i] = new Thread(accountOperator, "Thread" + i); threads[i].start(); &#125; &#125;&#125; 运行结果： 12345Thread4:10500.0Thread0:10500.0Thread3:10500.0Thread2:10500.0Thread1:10500.0 这里出现了异常！我每次在同一个方法中存500，取500，输出应该是10000，而不是10500，很有可能的是，在存钱命令执行完毕之后，输出语句先分配到了内存进行了执行，之后才执行了取钱操作 我们加上锁看看： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package concurrency;/** * @author :DengSiYuan * @date :2019/3/15 8:59 * @desc : */public class Account &#123; private String name; private float amount; public Account(String name, float amount) &#123; this.name = name; this.amount = amount; &#125; //存钱 public void deposit(float amt) &#123; amount += amt; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //取钱 public void withdraw(float amt) &#123; amount -= amt; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public float getBalance() &#123; return amount; &#125;&#125;/** * 账户操作类 */class AccountOperator implements Runnable&#123; private Account account; public AccountOperator(Account account) &#123; this.account = account; &#125; @Override public void run() &#123; synchronized (account) &#123; account.deposit(500); account.withdraw(500); System.out.println(Thread.currentThread().getName() + ":" + account.getBalance()); &#125; &#125; public static void main(String[] args) &#123; Account account = new Account("小明", 10000.0f); AccountOperator accountOperator = new AccountOperator(account); final int THREAD_NUM = 5; Thread threads[] = new Thread[THREAD_NUM]; for (int i = 0; i &lt; THREAD_NUM; i ++) &#123; threads[i] = new Thread(accountOperator, "Thread" + i); threads[i].start(); &#125; &#125;&#125; 演示结果： 12345Thread0:10000.0Thread4:10000.0Thread3:10000.0Thread2:10000.0Thread1:10000.0 这里我们对帐户进行了加锁处理，我们就会发现这里的结果是正确的，在存钱的线程进行完毕释放锁之后，取钱的线程才会获得锁的使用权，进行取钱操作！！ 当我们没有特别的加锁对象的时候，只是为了让一段代码实现同步，为了减少开销，我们会使用private byte[] lock = new byte[0];作为锁进行处理 ¶修饰一个方法 Synchronized修饰一个方法很简单，就是在方法的前面加synchronized，public synchronized void method(){}; synchronized修饰方法和修饰一个代码块类似，只是作用范围不一样，修饰代码块是大括号括起来的范围，而修饰方法范围是整个函数。如将【Demo1】中的run方法改成如下的方式，实现的效果一样。 ¶Demo4 12345678910public synchronized void run() &#123; for (int i = 0; i &lt; 5; i ++) &#123; try &#123; System.out.println(Thread.currentThread().getName() + ":" + (count++)); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; ¶修饰方法的两种写法 ¶写法一： 修饰在方法返回参数之前 123public synchronized void method()&#123;&#125; ¶写法二： 修饰在方法内，将方法中的所有代码包含起来 12345public void method()&#123; synchronized(this) &#123; &#125;&#125; 写法一和写法二是等价的，都是锁定了整个方法内部的内容 ¶修饰方法时需要注意的 synchronized关键字不能继承 虽然可以使用synchronized来定义方法，但synchronized并不属于方法定义的一部分，因此，synchronized关键字不能被继承。如果在父类中的某个方法使用了synchronized关键字，而在子类中覆盖了这个方法，在子类中的这个方法默认情况下并不是同步的，而必须显式地在子类的这个方法中加上synchronized关键字才可以。当然，还可以在子类方法中调用父类中相应的方法，这样虽然子类中的方法不是同步的，但子类调用了父类的同步方法，因此，子类的方法也就相当于同步了。这两种方式的例子代码如下： 123456class Parent &#123; public synchronized void method() &#123; &#125;&#125;class Child extends Parent &#123; public synchronized void method() &#123; &#125;&#125; 在子类方法中调用父类的同步方法 123456class Parent &#123; public synchronized void method() &#123; &#125;&#125;class Child extends Parent &#123; public void method() &#123; super.method(); &#125;&#125; 在定义接口方法时不能使用synchronized关键字 构造方法不能使用synchronized关键字，但可以使用synchronized代码块来进行同步。 ¶修饰一个静态方法 我们知道静态方法是属于类的而不属于对象的。同样的，synchronized修饰的静态方法锁定的是这个类的所有对象。 ¶Demo5 12345678910111213141516171819202122232425262728293031323334353637/** * @author :DengSiYuan * @date :2019/3/15 9:42 * @desc : */public class SyncThread implements Runnable &#123; private static int count; public SyncThread() &#123; count = 0; &#125; public synchronized static void method() &#123; for (int i = 0; i &lt; 5; i ++) &#123; try &#123; System.out.println(Thread.currentThread().getName() + ":" + (count++)); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; @Override public synchronized void run() &#123; method(); &#125; public static void main(String[] args) &#123; SyncThread syncThread1 = new SyncThread(); SyncThread syncThread2 = new SyncThread(); Thread thread1 = new Thread(syncThread1, "SyncThread1"); Thread thread2 = new Thread(syncThread2, "SyncThread2"); thread1.start(); thread2.start(); &#125;&#125; 执行结果： 12345678910SyncThread1:0SyncThread1:1SyncThread1:2SyncThread1:3SyncThread1:4SyncThread2:5SyncThread2:6SyncThread2:7SyncThread2:8SyncThread2:9 syncThread1和syncThread2是SyncThread的两个对象，但在thread1和thread2并发执行时却保持了线程同步。这是因为run中调用了静态方法method，而静态方法是属于类的，所以syncThread1和syncThread2相当于用了同一把锁。这与Demo1是不同的。 ¶修饰一个类 12345678910111213141516171819202122232425262728293031323334public class ClassThread implements Runnable &#123; private static int count; public ClassThread() &#123; count = 0; &#125; public static void method() &#123; synchronized(SyncThread.class) &#123; for (int i = 0; i &lt; 5; i ++) &#123; try &#123; System.out.println(Thread.currentThread().getName() + ":" + (count++)); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; @Override public synchronized void run() &#123; method(); &#125; public static void main(String[] args) &#123; ClassThread classThread1 = new ClassThread(); ClassThread classThread2 = new ClassThread(); Thread thread1 = new Thread(classThread1, "classThread1"); Thread thread2 = new Thread(classThread2, "classThread2"); thread1.start(); thread2.start(); &#125;&#125; 运行结果: 12345678910classThread1:0classThread1:1classThread1:2classThread1:3classThread1:4classThread2:5classThread2:6classThread2:7classThread2:8classThread2:9 其效果和【Demo5】是一样的，synchronized作用于一个类T时，是给这个类T加锁，T的所有对象用的是同一把锁。 ¶总结 A. 无论synchronized关键字加在方法上还是对象上，如果它作用的对象是非静态的，则它取得的锁是对象；如果synchronized作用的对象是一个静态方法或一个类，则它取得的锁是对类，该类所有的对象同一把锁。 B. 每个对象只有一个锁（lock）与之相关联，谁拿到这个锁谁就可以运行它所控制的那段代码。 C. 实现同步是要很大的系统开销作为代价的，甚至可能造成死锁，所以尽量避免无谓的同步控制。 @GuardedBy 了解到前面的sychronized之后，偶然间看到了GuardedBy这个注解，它可以去替代sychronized关键字去加锁。 官方文档 ¶Demo1 1234public class foo &#123; @GuardedBy("this") public String str;&#125; 该注解对String str进行了加锁，我们下面介绍以下该注解的参数都有哪些 ¶参数含义 this：在其类中定义字段的对象的固有锁 class-name.this：对于内部类，可能有必要消除“this”的歧义；class-name.this指定允许您指定“this”引用的意图 itself： 仅供参考字段; 字段引用的对象 field-name：锁对象由字段名指定的（实例或静态）字段引用。 class-name.field-name：锁对象由class-name.field-name指定的静态字段引用 method-name()：锁对象通过调用nil-ary方法 class-name：指定类的Class对象用做锁定对象 ¶Demo2 123456public class BankAccount &#123; private Object credential = new Object(); @GuardedBy("credential") private int amount;&#125; 在上面的代码片段中，当有人获得了凭据的同步锁定时，可以访问金额，因此，BankAccount中的金额由凭据保护。 让我们给这个类添加一些东西. 123456789public class BankAccount &#123; private Object credential = new Object(); @GuardedBy("credential") private int amount; @GuardedBy("listOfTransactions") private List&lt;Transaction&gt; listOfTransactions;&#125; 我们现在有一个在BankAccount的事务列表。 List对象有许多元素，因此它具有对列表中所有元素的引用。 这里我们使用@GuardedBy(“listOfTransactions”)来指定锁与listOfTransactions所引用的对象相关联。 换句话说，有人必须持有所有事务的锁，以便保持此List对象的锁定。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop基础架构介绍]]></title>
    <url>%2Fposts%2Fe29dca0d.html</url>
    <content type="text"><![CDATA[版本3.0以后的Hadoop架构图 Hadoop的核心（一） ----HDFS hdfs是Hadoop的核心，是一个分布式文件系统，可以存储海量数据 ¶基本概念 ¶块 在Hadoop1.0-2.7.3版本中，默认大小为64M，之后的默认大小为128M，在Hadoop中，文件都被切成这样的块进行存储。 ¶NameNode 管理节点，存放文件元数据 文件与数据块的映射关系 数据块与数据节点的映射表 ¶DataNode 工作节点，存放数据块，数据的实际存储位置 ¶HDFS架构图 这里的SecondaryNameNode是NameNode的备份，一般DataNode会备份（三份），HDFS会进行心跳检测，当一个节点挂掉之后，会再去复制一份 ¶HDFS读取流程 ¶HDFS写入流程 ①将文件拆分成块 ②查询NameNode，返回当前可用DataNode ③将分好的块写入DataNode ④流水线复制给其他备份 ⑤更新NameNode数据 ⑥更新NameNode备份SecondaryNode ¶HDFS特点 1、数据冗余，硬件容错（DataNode三备份，NameNode双备份） 2、流式数据访问 3、存储大文件（小文件成本大，尽量不存储，效率低下） ¶适用性 1、数据批量读写，吞吐量高 2、一次写入，多次读写，顺序流水读写 ¶局限性 1、延迟高，不适合交互式应用（不如关系型数据库/非关系型数据库） 2、不支持多用户并发写相同的内容 Hadoop的核心（二） ----MapReduce ¶什么是MapReduce MapReduce是面向大数据并行处理的计算模型、框架和平台，包含以下几个方面的解读： （1）MapReduce是一个基于集群的高性能并行计算平台（Cluster Infrastructure）。它允许用市场上普通的商用服务器构成一个包含数十、数百至数千个节点的分布和并行计算集群。 （2）MapReduce是一个并行计算与运行软件框架（Software Framework）。它提供了一个庞大但设计精良的并行计算软件框架，能自动完成计算任务的并行化处理，自动划分计算数据和计算任务，在集群节点上自动分配和执行任务以及收集计算结果，将数据分布存储、数据通信、容错处理等并行计算涉及到的很多系统底层的复杂细节交由系统负责处理，大大减少了软件开发人员的负担。 （3）MapReduce是一个并行程序设计模型与方法（Programming Model &amp; Methodology）。它借助于函数式程序设计语言Lisp的设计思想，提供了一种简便的并行程序设计方法，用Map和Reduce两个函数编程实现基本的并行计算任务，提供了抽象的操作和并行编程接口，以简单方便地完成大规模数据的编程和计算处理 . ¶MapReduce能做什么？原理？ MapReducce模型适合处理大数据 处理数据的思想分而治之&quot;,将一个任务分成多个小任务，然后分开执行处理，最后再将得到的结果 ¶原理 ¶（1）Mapper负责“分” mapper将一个大任务，分成一个个的小任务（“简单的任务”）： 数据、计算规模比原来的小很多 就近计算原则，任务会放到其所需数据的节点上进行执行，减少调度数据的时间 小任务之间不会相互干扰，并行执行计算 ¶（2）Reduce负责“合” Reduce负责将各个map计算的结果进行合并汇总 MapReduce的整个工作过程如上图所示，它包含如下4个独立的实体： 实体一：客户端，用来提交MapReduce作业。 实体二：JobTracker，用来协调作业的运行。 实体三：TaskTracker，用来处理作业划分后的任务。 实体四：HDFS，用来在其它实体间共享作业文件。 ¶节点Map任务的个数 数据从DataNode中取出后需要进行split进行分片操作，操作大致如下： 一个DataNode的Map任务数量10~100个最宜 ¶控制Map数量的方法 增加Map个数，可以去增大mapred.map.tasks 减少Map个数，可以去增大mapred.min.split.size 小文件需要先合，在使用上一方法进行减少map个数 ¶控制Reduce数量的方法 调节mapred,reduce.tasks 代码中调用job.setNumReduceTasks(int n) ¶执行流程 ¶本地优化----Combine 逻辑和Reduce一致，也可以自己实现Combiner类来进行本地优化。 数据经过Map端输出后会先进行网络混洗，经过Shuffle后进入Reduce，大数据的情况下会产生大量的网络开销。因此在本地会先按照Key进行一轮的排序和合并，再给下一层进行混洗，这一过程就是Combine。 ¶split----Map----Shuffle----Reduce ¶异常 若出现问题，数据拿不到： MapReduce会重复执行四次，重新获取数据，排除DataNode的问题 重新开一个相同的任务，排除MapReduce的问题]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop的安装和配置]]></title>
    <url>%2Fposts%2F1ce5e607.html</url>
    <content type="text"><![CDATA[Hadoop的安装 ¶安装环境 ¶Linux系统 这里需要一台装有Linux系统的服务器或客户机 ¶JDK环境 ¶查看已有环境 1$ java 若提示无该命令，即该电脑还未安装配置JDK，需要我们去自己安装。 ¶安装JDK 1$ apt-get install openjdk-8-jdk 下载完成后，需要我们像Windows系统一样去配置环境变量 ¶配置环境变量 1$ vim /etc/profile 我们使用vim编辑器进入该文件内，在文末添加以下几句话 1234export JAVA_HOME=这里是你安装JDK的目录export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATHexport PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH 这里我们保存退出，输入以下命令，使系统重新加载该文件 1$ source /etc/profile 重新加载完该文件后，我们继续输入JAVA进行测试，若出现帮助文档，即证明我们已经配置完成。 ¶安装Hadoop ¶下载压缩包 我们使用wget命令去获取Hadoop的压缩包(我这里使用的是3.2.0版本) 1$ wget http://mirrors.hust.edu.cn/apache/hadoop/core/stable/hadoop-3.2.0.tar.gz ¶解压 将该压缩包移动到自己想放的地方，然后进行解压，我这里选择opt目录下 12$ mv hadoop-3.2.0.tar.gz /opt/$ tar -zxvf hadoop-3.2.0.tar.gz ¶文件结构 hadoop bin hadoop.sh hdfs.sh mapred.sh yarn.sh etc hadoop（内部是各种配置文件，重要的包括以下几个） core-site.xml hdfs-site.xml hadoop-env.sh mapred-site.xml yarn-site.xml include lib libexec logs(存放用户操作日志) sbin（bin中很多模块的启动和终止脚本） start-all.sh stop-all.sh start-dfs.sh start-yarn.sh share doc（一些使用文档说明书，用户手册） hadoop（存放了一些操作必须的Jar包） ¶修改配置 文件结构中，看到配置文件主要放置在etc目录下的hadoop目录中 ¶etc/core-site.xml 新装的hadoop没有任何配置，这里面是空的我们需要添加下面的配置 12345678&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/hadoop&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://geniusdsy-PC:9999&lt;/value&gt; &lt;/property&gt; 这里的hadoop.tmp.dir是hadoop的工作目录，存储我们使用hadoop时产生的所有数据 这里的fs.default.name是hadoop服务的启动端口（geniusdsy-PC就是本机127.0.0.1） ¶etc/hdfs-site.xml 1234&lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/hadoop/data&lt;/value&gt; &lt;/property&gt; 这里是配置hdfs分布式文件管理系统在运行时产生的所有数据存放的地方，我将它放在刚才设置的hadoop数据大目录下的一个data文件夹内 ¶etc/yarn-site.xml 12345678&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; ¶etc/hadoop-env.xml 1export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 在该xml文件中找到这一句，将前面的注释删掉 接下来我们需要配置一下启动项，将权限规定一下 ¶sbin/start-dfs.sh &amp; sbin/stop-dfs.sh 1234HDFS_DATANODE_USER=root HDFS_DATANODE_SECURE_USER=hdfs HDFS_NAMENODE_USER=root HDFS_SECONDARYNAMENODE_USER=root ¶sbin/start-yarn.sh &amp; sbin/stop-yarn.sh 123YARN_RESOURCEMANAGER_USER=rootHDFS_DATANODE_SECURE_USER=yarnYARN_NODEMANAGER_USER=root ¶配置Hadoop的环境变量 我们去配置一下环境变量，这样我们就可以在任何地方使用Hadoop的脚本，还是那个文件 1$ vim /etc/profile 在里面需要改变的地方有两个 12export HADOOP_HOME=/opt/hadoopexport PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH 记得改完之后重新加载一下该文件 这里我们就将基本的安装和基本的配置完成了，接下来我们去启动一下 ¶启动Hadoop ¶对namenode进行格式化 12342.0以上版本$ hdfs namenode -format2.0以下版本$ hadoop namenode -format ¶启动！！！！！ 刚才我们是不是在sbin目录下看到了一个start-all文件，通过它可以启动Hadoop里面的所有模块,并且我们也刚才把他们加入了环境变量中，我们在任何位置都可以使用这些脚本了 1$ start-all 等待片刻，有时候需要我们去输入密码，我们按他的指示操作完之后，输入jps进行查看是否启动成功 当我们看到下面的显示时，证明我们启动成功了 下一篇，记录一下Hadoop的架构介绍，使用原理]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux入门之认识Shell（四）]]></title>
    <url>%2Fposts%2F8c991ca6.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Linux入门之认识Shell（三）]]></title>
    <url>%2Fposts%2F3bb405c3.html</url>
    <content type="text"><![CDATA[重定向 命令行最酷的功能----IO重定向。我们可以将需要输入命令行的东西重定向到从文件中获取，也可以将输出内容输出到文件中。我们如果可以将多个命令行结合起来，那么就形成了强大的命令----管道 ¶标准输入、标准输出、标准错误 ¶标准输出重定向 IO重定向功能可以重新定义标准输出内容发送到哪里。使用重定向操作符“&gt;”，后面接文件名，就可以将标准输出输出到指定文件中，而不显示在屏幕上。 ¶Demo1 输出到屏幕上 输出到文件中 我们去看看文件 和之前屏幕上输出的一模一样 ¶Demo2 我们改一下，试一下将一个不存在的目录信息输出到文件中 演示实例 这时候我们看到，这里输出了错误的信息，接下来我们看到刚才输出文件的大小变成了0.原来错误信息输出到了屏幕，这时候输出文件中什么都没有写入，并且因为默认的写入方式是从头写入。 ✔️Get到了新技能🎉🎉: ​ 我们新建一个文件或者要删除一个文件里面所有内容时，我们可以直接使用 1$ &gt; ls-output.txt 演示实例 ¶Demo3 可是我们有时候就是需要将一些数据加到一个文件的末尾，而我们用上面的指令就会从头覆盖掉？？那我们该怎么办呢？？（手动头大🤕🤕) 我们可以使用下面的指令对他进行写入 1$ ls -l /usr/bin &gt;&gt; ls-output.txt 我们去看看结果 ¶标准错误重定向 一个程序可以把他生成的输出内容输出到任意文件流中。这些文件流中的前三个分别对应标准输入文件、标准输出文件和标准错误文件，shell将在内部用文件描述符分别索引它们为0、1和2 所以可以用以下的指令进行将标准错误重定向到一个文件中 1$ ls -l /bin/usr 2&gt; ls-error.txt 结果图 ¶将标准输出和标准错误重定向到同一个文件 我们在写好一个程序之后，我们希望将他运行时的很多输出信息输出到一个文档中去，以便我们在查看他的运行状况时，能够很方便。为此，我们必须同时重定向标准输出和标准错误。介绍以下两种方法 ¶第一种：传统方法，在老版shell中使用 1$ ls -l /bin/usr &gt; ls-output.txt 2&gt;&amp;1 这个方法，将执行两个重定向操作。 1️⃣重定向标准输出到ls-output.txt文件中 2️⃣使用标记符2&gt;&amp;1把文件描述符2（标准错误）重定向到文件描述符1（标准输出）中 ¶第二种：新版Bash高效方法 1$ ls -l /bin/usr &amp;&gt; ls-out-put.txt ¶标准输入重定向 ¶cat----合并文件 ¶读取文件 cat命令读取一个或多个文件，并把他们赋值到标准输出文件中，格式如下： 1$ cat [file...] ¶拼接文件 大多数情况下cat显示的文件不会进行分页，我们可以用它来显示短一些的文本文件，或者将一些文件进行拼接显示，中间不会有分页，直接连接在一起。我来试一下 1$ cat ls-output.txt ls-output1.txt 运行结果 ¶拼接文件并重定向输出到新的文件 准备好三个文件 ls-output0.txt ls-output1.txt ls-output2.txt 使用下面的命令 1$ cat ls-output*.txt &gt; ls-output.txt 运行结果 ¶创建文件并写入内容 12$ cat &gt; pencil.txtThis is a pencil 我们在输入完第一句话的时候，就会发现这时候当前目录下多了一个文件，但是该行命令并没有结束。这时候我们输入我们想要存在该txt中的内容，之后按Ctrl+D结束。 1$ cat pencil.txt 结果： ¶管道 命令从标准输入到读取数据，并将数据发送到标准输出的能力，是使用了名为管道的shell特性。管道操作符“|”可以把一个命令的标准输出传送到另一个命令的标准输入中。 指令格式 1$ command1 | command2 演示： 1$ ls -l /usr/bin | less 这里将前面的标准输入交给后面的less，less接收之后进行分页展示的他的标准输入 ¶过滤器 管道经常可以对数据执行一系列的复杂操作，就相当于一大段数据，从管道的一个口进去，这个管道是有多条指令组成，从管道口到管道尾的时候就会进行这一系列指令的操作，最终输出这一系列操作的结果数据。 1$ ls /bin /usr/bin | sort | less 结果图 形象解释 ¶uniq----报告或忽略文件中重复的行 uniq经常会和sort结合使用。uniq命令接收来自于标准输入或者是已经排好序的数据列表。uniq会删除所有的重复行。 1$ ls /bin /usr/bin | sort | uniq | less 这样的输入中与上面不同的就是会少了很多重复行 但若我们想看那些行都重复了，展示重复行的信息，那我们就可以在uniq后面将上一个选项-d进行输出 1$ ls /bin /usr/bin | sort | uniq -d | less 结果图 这次我们看到的就是这标准输入流中的重复项 ¶wc----打印行数、字数和字节数 wc命令可以用来显示文件中包含的函数、字数和字节数（word count） 演示代码 1$ wc pencil.txt 结果图 我们也可以加上wc的选项进行筛选，只输出其中的行数/字数/字节数 演示结果图 选项 含义 -l 行数 -w 字数 -m 字符数 -c 字节码数 -L 最大宽度 ¶grep----打印匹配行 grep是一个强大的功能，它可以打印出包含该文本的文件。它可以匹配的内容非常的复杂，现在学会文本就行。 匹配所有文件中包含zip的所有文件 1$ ls /bin /usr/bin | sort | uniq | grep zip 演示结果 grep存在以堆方便的选项：-i ，该选项使得grep在搜索时忽略大小写；-v ，该选项使得grep只输出和模式不匹配的行 ¶head/tail----打印问价的呢开头部分/结尾部分 有时候只需要输出开头的几行或者末尾的几行，那么我们可以使用这两个指令进行简约化输出。默认输出10行，我们可以使用选项-n进行选择输出行数的参数。 前5行 1$ head -n 5 ls-output.txt 后5行 1$ tail -n 5 ls-output.txt 当然这些命令我们也可以使用在管道中 哇塞，这么长的命令居然只给我输出一个5？？？？我5555.。。。看看这个管道的原理吧 看到了吗？？？？进去的数据到最后就是筛选出来的那个文件的单词数量了！！！！可见我们的管道是多么好的隐藏处理数据的工具啊！！ ¶tee----从stdin读取数据，并同时输出到stdout和文件 tee命令就像我们在管道上插了一个“T”在管道上，tee指令所在的地方，将会把上面传来的指令进行输出，并且将这些指令继续向下传输，感觉就像是为了看这一步的数据做他用或者只是单纯的记录 1$ ls /usr/bin | tee ls-output.txt | grep zip 这里我们会将上一步得到的数据进行输出到ls-output.txt，但不影响下一步的操作，这个数据不会中断，任然会传到下一个操作处。 此文结束。。。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux入门之认识Shell（二）]]></title>
    <url>%2Fposts%2F76aaa691.html</url>
    <content type="text"><![CDATA[操作文件与目录 ¶通配符 我们在查找一些文件或者一些目录时，需要找到符合条件的所有文件，这时候就需要模糊查询。在模糊查询中，必不可少的就是通配符的使用。 表 通配符 通配符 匹配项 * 匹配任意多个字符（包括0个和2个） ? 匹配任一单个字符（不包括0个） [characters] 匹配任意一个属于字符集中的字符 [!characters] 匹配任意一个不属于字符集中的字符 [[:class:]] 匹配任意一个属于指定字符类中的字符 表 常用字符类 字符类 匹配项 [:alnum:] 匹配任意一个字母或数字 [:alpha:] 匹配任意一个字母 [:digit] 匹配任意一个数字 [:lower:] 匹配任意一个小写字母 [:upper:] 匹配任意一个大写字母 **字符范围** 在Unix环境中，可能会遇到[A-Z]或[a-z]形式的字符范围表示法。这些是传统的Unix表示法，在早期版本的Linux仍然起作用，但在现在，使用时务必小心，可能会造成位置的严重后果。因此，我们需要避免使用，而是使用字符集 ¶mkdir----创建目录 格式如下： 1$ mkdir directory1 directory2 可以同时创建多个目录 ¶cp----复制文件和目录 具有两种不同的用法： ¶将单个文件或者目录item1复制到文件或目录item2中 格式如下: 1$ cp item1 item2 演示结果 ¶将多个项目复制到一个目录中 格式如下: 1$ cp item1 item2 directory 表 cp命令选项 选项 含义 -a,–archive 复制文件和目录及其属性，包括所有权和权限。通常来说，赋值的文件具有用户所操作文件的默认属性 -i,–interactive 在覆盖一个已存在的文件前，提示用户进行确认。如果没有指定该选项，cp会默认覆盖 -r,–recursive 递归地复制目录及其内容。赋值目录时需要这个选项(或-a选项) -u,–update 当将文件从一个目录复制到另一个目录时，只会复制那些目标目录中不存在的文件或时目标目录相应文件的更新文件 -v, --verbose 复制文件是，显示信息性消息 ¶mv----一处和重命名文件 mv命令可以执行文件移动以及文件重命名操作，完成操作后，原来的文件名将不存在，他的用法与cp指令基本相似 将文件（或目录）item1移动（或重命名）为item2 1$ mv item1 item2 将一个或多个条目从一个目录移动到另一个目录下 1$ mv item... directory 表 mv选项 选项 含义 -i, --interactive 覆盖一个存在的文件之前，提示用户确认。未指定该选项，则会默认覆盖 -u, --update 将文件从一个目录移动到另一个目录，只移动那些目标目录中不存在的文件或是目标目录里相文件的更新文件 -v, --verbose 移动文件是显示信息性消息 演示结果 ¶rm----删除文件和目录 rm命令用来删除文件和目录 1$ rm item... 小心rm命令 在类Unix系统中，并不包含还原删除操作的命令。一旦使用rm命令删除掉，就彻底删除了。Linux认为操作用户是明智的，清楚自己再做些什么。 rm命令与通配符在一起时要尤其小心，如下所示： rm *.html 该指令会删掉所有后缀是html的文件，但若是在*与.html之间多打了一个空格，即: rm * .html 那么系统就会删掉所有文件，然后告诉用户没有找到叫做.html的文件 **提示**： 在使用rm指令时，可以先使用ls指令显示出自己要删除的文件，确认之后在使用rm指令删除掉。 表 rm选项 选项 含义 -i, --interactive 删除一个已存在的文件前，提示用户确认（最好养成习惯加上该选项） -r, --recursive 递归地删除目录，将所选目录及其所有子目录一并递归删除（删除一个目录是必须指定该项） -f, --force 忽略不存在的文件并无需确认。该选项的优先级高于-i -v, --verbose 删除文件是显示信息性消息 演示结果 ¶ln----创建链接 ¶硬链接 硬链接是最初Unix创建链接的方式。默认情况下，每个文件有一个硬链接，该硬链接会给文件起名字。当创建一个硬链接的时候，会为这个文件创建一个额外的目录条目。 局限性 硬链接不能引用自身文件系统之外的文件。也就是说，链接不能引用与该链接不再同意磁盘分区的文件 硬链接无法链接目录 创建硬链接 1$ ln file link ¶符号链接 符号链接是为了克服硬链接的局限性而创建的。通过创建一个特殊类型的文件来起作用的，该文件包含了只想引用文件或目录的文本指针。类似于Windows中的快捷方式 创建符号链接 1$ ln -s item link 命令的使用 ¶什么是命令 一条命令不外乎以下4种情况： 可执行程序 可执行文件就像在/usr/bin目录里的那些文件一样。在该程序类别中，程序可以编译为二进制文件，比如C、C++语言编写的程序，也可以是shell、Python、Ruby等脚本语言编写的程序。 shell内置命令 bash支持许多内部称之为shell builtin的内置命令。例如：cd shell函数 shell函数是合并到环境变量中的小型shell脚本 alias命令 在其他命令的基础上定义自己的命令（有点像自定义命令❔❔） ¶识别命令 我们要去知道一些命令属于上面四种的哪一个，要用一些方法去辨别这些命令，Linux给我们提供了两个方法来识别命令类型。 ¶type----显示命令的类型 type是一个shell内置命令，可以根据指定的命令名显示shell将要执行的命令类型。 格式如下： 1$ type command 演示实例 ¶which----显示可执行程序的位置 在大型服务器中，可能会安装多个版本的可执行程序。使用which可以准确的显示该程序所处的准确位置。 1$ which ls 演示实例 which只适用于可执行程序，其他种类的指令，只会无响应或者得到错误信息 ¶获得命令文档 每一类指令都会有文档作为他的使用须知，我们可以去阅读这些文档获得他的说明以及使用方法 ¶help----获得shell内置命令的帮助文档 bash为每一个shell内置命令提供了一个内置的帮助文档。输入help然后输入shell内置命令的名称即可使用该帮助文档。 演示实例 ¶help----显示命令的使用信息 在可执行程序中，我们可以在其后跟上一个–help的选项进行获取帮助文档 演示实例 即使有一些命令不支持–help选项，但是我们也应该去试一试，会获得错误信息以及相似命令的使用方法 ¶man----显示程序的手册页 大多数供命令行使用的可执行文件们提供一个陈志伟manual或者是man page的正式文档（有点像javadoc api，开发者手册），这些文档可以使用一个叫做man的特殊分页程序来查看 演示实例 表 手册文档的组织结构 部分 内容 1 用户命令 2 内核系统调用的程序接口 3 C库函数程序接口 4 特殊文件，如设备节点和驱动程序 5 文件格式 6 游戏和娱乐，例如：屏幕保护程序 7 其他杂项 8 系统管理命令 以文件格式展示passwd内容 1$ man 5 passwd 演示实例 ¶apropos----显示合适的命令 我们可能会去搜索手册列表，进行某个搜索条目的匹配。 演示实例 ¶whatis----显示命令的简要描述 whatis程序显示匹配具体关键字的手册页的名字和一行描述 演示实例 ¶info----显示程序的info条目 GNU项目提供了info页面来代替手册文档。info页面可通过info阅读器显示。info页面使用超链接 ¶alias----创建属于自己的命令 我们可以通过alias来编写属于自己的命令。平时，我们可以使用分号将一些列动作输出在一行中。 例如： 1$ cd /usr; ls; cd - 演示结果 现在我们尝试将这三个命令组合起来，创建一个属于自己的命令 我们先使用type看一看我们自定义的名字有没有被其他指令所占用 我们选取self作为自定义命令的名称 创建新命令 指令结构 1$ alias name='组合指令' 查询并删除自定义指令 系统默认的别名命令 Linux入门之认识Shell（一）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识Redis（三）之数据结构]]></title>
    <url>%2Fposts%2Fac4e22e0.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[初识Redis（二）之Jedis]]></title>
    <url>%2Fposts%2F388421ef.html</url>
    <content type="text"><![CDATA[Jedis的了解与简单实用 ¶1、什么是Jedis Jedis是Redis官方首选的Java客户端开发包。它集成了Redis的指令操作，封装了Redis的Java客户端，提供了连接池管理。一般不直接使用jedis，而是在其上在封装一层，作为业务的使用。 GitHub地址：https://github.com/xetorthio/jedis ¶2、Jedis的引入 我们要在Java程序中使用Jedis，需要以下几个前提条件： 本机安装了Redis Redis下载地址：https://redis.io/download 下载所需要的Jedis的Jar包 手动导入Jar包 jedis-2.9.0.jar：http://central.maven.org/maven2/redis/clients/jedis/2.9.0/jedis-2.9.0.jar commons-pool2-2.4.2.jar: http://central.maven.org/maven2/org/apache/commons/commons-pool2/2.4.2/commons-pool2-2.4.2.jar 将刚才下载的Jar包导入 使用maven管理项目 12345678910&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt;&lt;/dependency&gt; 将以上代码copy到你的pom.xml文件中 ¶3、Jedis的第一次使用 使用控制台启动redis（这里是Windows系统进行演示） Java代码 123456789101112131415161718192021222324252627 package redis; import org.junit.Test; import redis.clients.jedis.Jedis;/** * @author :DengSiYuan * @date :2019/2/2 17:57 * @desc : 这是使用第一次使用jedis的测试 */public class JedisTest &#123; @Test public void demo1()&#123; //设置客户端启动的IP地址以及端口 Jedis jedis = new Jedis("127.0.0.1",6379); //2.保存数据 jedis.set("name","GeniusDSY"); //3.获取数据 String value = jedis.get("name"); System.out.println(value); //3.关闭 jedis jedis.close(); &#125;&#125; 运行结果图 ¶4、我们来试试连接虚拟机的Redis 我这里是用的deepin，默认防火墙关闭，若是其他系统，这里写一下解决方案 首先vim /etc/sysconfig/iptables 设置一下防火墙 进来之后找到上面那行，yy(复制) + p(粘贴)，改一下端口号 然后重启我们的防火墙 这里将防火墙这是好了，已经给外部开启了6379端口，但是你若尝试一下，还是不可以访问 去看看redis的配置文件 更改这里 还有这里 最后一步，查一下IP地址 将java代码更改为： 123456789101112131415161718192021222324252627package redis;import org.junit.Test;import redis.clients.jedis.Jedis;/** * @author :DengSiYuan * @date :2019/2/2 17:57 * @desc : 这是使用第一次使用jedis的测试 */public class JedisTest &#123; @Test public void demo1()&#123; //设置客户端启动的IP地址以及端口 Jedis jedis = new Jedis("192.168.200.129",6379); //2.保存数据 jedis.set("name","GeniusDSY"); //3.获取数据 String value = jedis.get("name"); System.out.println(value); //3.关闭 jedis jedis.close(); &#125;&#125; 运行结果 学会了Jedis的引入以及简单使用，那么我们后面就可以去看看他的其他用处，这里我将Jedis的API粘出来，供大家学习以及参考。 Jedis API]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>NoSql</tag>
        <tag>Redis</tag>
        <tag>Jedis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux入门之认识Shell（一）]]></title>
    <url>%2Fposts%2F46bc4a49.html</url>
    <content type="text"><![CDATA[一、认识Shell 我们嘴上常说命令行、命令行。真正的命令行其实就是shell，shell就是一个接受由键盘输入的命令，之后将其提交给操作系统进行执行的程序。 我们打开终端，就可以看见下面的场景： 简单解释一下这些含义： 用户名@机器名:当前工作目录$ 待输入的指令 而若并不存在$而是一个#代替了它，则代表当前是超级管理员在使用终端进行操作，具有超级管理员的权利 ¶1、简单的命令 查看当前系统时间和日期 查看当月的日历 查看磁盘驱动器已用、可用、挂载点 查看可用内存 退出终端 ¶2、文件系统 Linux系统中存在一个文件系统树，当然会涉及到很多的文件的操作指令，记录一下 查看当前工作目录 列出目录内容 一般情况 列出包含隐藏文件(以.开头的文件为隐藏文件) 更改当前工作目录 绝对路径名 相对路径名 一般在任何情况下都可以省略./，即： 切换到主目录 返回到上一次的工作目录 将工作目录更改为username所在的主目录 ¶3、Linux系统 ¶（1）最常用的命令 —ls 只用输入一个ls即可知道该目录下的文件以及子目录 也可以显示指定目录下的文件以及子目录 也可以改变输出格式显示更多的细节（-l 以长格式显示） ¶①选项和参数 大部分命令都一个命令带有一个或多个选项，再之后回答有一个或多个参数，这些参数时命令作用的对象 1command -options arguments 例如: 1$ ls -lt -lt：这里的l表示以长格式形式输出，以展示详细信息，这里的t代表将这些按时间进行排序（从现在到过去）。 1$ ls -lt --reverse –reverse：这里的–reverse是参数，表示将-lt按时间排出来的信息反向输出（从过去到现在） ls命令的常用选项 选项 长选项 含义 -a –all 列出所有文件，包括一点好开头的文件，这些文件通常是不列出来的（比如隐藏的文件） -d –directory 可以与-l结合（-ld或者-l --directory）展示该父目录的详细信息，而不是-l时的该目录下文件和子目录的详细信息 -F –classify 该选项会在列出的每个名字后面加上类型指示符（若是一个目录则会加上斜杠） -h –human-readable 长格式形式输出，以人们可读的形式显示文件大小而不是字节数 -l 使用长格式显示结果 -r –reverse 以相反结果显示结果，通常ls是按照字母升序进行排序的 -S 按文件大小对文件进行排序（降序） -t 按文件修改时间 DEMO ls ls -a ls --all ls ls -l ls -ld ls -l --directory ls ls -l ls -lh ls -l --human-readable ls ls -l ls -lr ls -l --reverse ls -l ls -lS ls -l ls -lt ls -lt --reverse ¶②长格式简介 -l输出的长格式中有很多信息，图如下： 字段 含义 -rw-r-r– 对文件的访问权限。第一个字符表示文件的类型：“-”表示是一个普通文件，d表示目录。之后的三个字表示文件所有者的访问权限，再接着三个字母表示文件所属组中成员的访问权限，最后三个字母表示其他所有人的访问权限。详细解释见此👉待补全 1 文件硬链接数目。链接的内容在本文后端 root 文件所有者的用户名 root 文件所属用户组的名称 4096 以字节数表示文件的大小 12月 12 20：25 上次修改文件的时间 src 文件名 ¶（2）使用file命令确定文件类型 我们在使用linux，拿到一个陌生的文件，这时候知道他的内容就非常有用了。为此，我们可以使用该命令进行查看文件类型。 1$ file filename 结果图 ¶（3）使用less命令查看文件内容 less是一种查看文本文件的程序的命令。通过less命令我们可以更好的去了解文本文件的内容，更好的去了解一些配置文件的内容，了解系统一节各种文件是如何运作的。 1$ less filename 我们可以查看文本内容，按Q键可以退出less程序 1$ less /etc/passwd 演示结果图 less命令 命令 功能 PAGE UP或b 后翻一页 PAGE DOWN或Spacebar 前翻一页 向上箭头键 向上一行 向下箭头键 向下一行 G 跳转到文本文件的末尾 1G或g 跳转到文本文件的开头 /charecters 向前查找指定的字符串 n 向前查找下一个出现的字符串，这个字符串是之前所指定查找的 h 显示帮助屏幕 q 退出less ¶（4）Linux的基础目录了解 目录 内容 / 根目录，一切从这里开始 /bin 包含系统启动和运行所必需的二进制文件（程序）包含Linux内核、最初的RAM磁盘映像（系统启动时，驱动程序会用到），以及启动加载程序 /boot 有趣的文件：🔼 /boot/grup/grup.conf或者menu.lst，用来配置启动加载程序🔼 /boot/vmlinuxz,linux内核 /dev 这是一个包含设备节点的特殊目录。“把一切当成文件”也适用于设备。内核将所有可以识别的设备存放在这个目录内 /etc 有趣的文件：这里/etc里面的文件目录里面的任何文件都很有趣，这里仅列出一些：🔼 /etc/crontab,该文件定义了自动化任务运行的时间🔼 /etc/fstab,存储设备以及相关挂载点的列表🔼 /etc/passwd,用户账号列表 /home 在通常的配置中，每个用户都会在/home目录中拥有一个属于自己的目录。普通用户只能在自己的主目录中创建文件。这一限制可以实现各用户之间的独立性，保护系统免遭错误的用户行为的破坏 /lib 包含核心系统程序使用的共享文件库。类似于Windows中的DLL /lost+found 内阁使用Linux文件系统的格式化分区或设备，当文件系统崩溃时。该目录用于恢复分区。除非系统发生很严重的问题，否则这个目录一直是空的 /media 在现代Linux系统中，/media目录包含可以出媒体设备的挂载点。比如USB驱动、CD-ROM等。这些设备在插入计算机后，就会自动挂载到这个目录节点下 /mnt 在早期的Linux系统中，/mnt目录包含手动挂在的可以出设备的挂接点 /opt /opt目录用来安装其他可选的软件，主要用来存放可能安装在系统中的商业软件 /proc /proc目录很有特殊.从文件的角度来说，它不是存储在硬盘中的真正的文件系统，反而是一个linux内核维护的虚拟文件系统。它包含的文件是内核的窥视孔。该文件是可读的，从中可以看到内核是如何监管计算机的。 /root root账户的主目录 /sbin 该目录存放“系统”二进制文件，这些程序执行重要的系统任务，这些任务通常是为超级用户预留的 /tmp /tmp是供用户存放各类程序创建的临时文件的目录。某些配置文件是的系统重启时都会清空该目录 /usr 可能是linux中最大的目录树。它包含普通用户使用的所有程序和相关文件 /usr/bin 防止一些Linux返航版安装的可执行文件。该目录会存储成千上万个程序 /usr/lib 该目录中的程序使用的共享库 /usr/local 这个目录是并非系统发行版自带，但却打算让系统使用的程序的安装目录，有段代码编译好的程序通常安装在/usr/local/bin中。在一个新安装的Linux系统中，就存在着一个目录，但却是空目录，直到系统管理员向其中添加内容 /usr/share 该目录里面包含了/usr/bin中的程序所适用的全部共享数据，这包括默认配置文件、图标、屏幕背景、音频文件等 /usr/share/doc 安装在系统中的大部分程序包包含一些文档文件。在/usr/share/doc中文档文件是按照软件包类组织分类的 /var 除了/tmp和/home目录之外，模卡能看到的目录相对来说都是静态的；也就是说，其中包含的内容是不变的。而那些可能改变的数据存储在/var目录梳理。各种数据库、见多将文件、用户邮件等都存在这里 /var/log 该目录包含日志文件，记录了各种系统活动，这些文件非常重要，并且应该时不时的监控他们。其中最有用的文件是/var/log/messages。注意，为了安全起见，很多系统里面，只有超级用户才能查看日志文件 Linux入门之认识Shell（二）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识Docker（一）之安装]]></title>
    <url>%2Fposts%2F6507e5e0.html</url>
    <content type="text"><![CDATA[今天在虚拟机上乱搞，准备使用redis的端口开放给本机使用，结果防火墙没搞好，把之前的Docker给搞崩了，又只能卸载掉，重新装了一下，记录一下本次安装的过程，谨防下次搞崩重装，却不知所措。 安装Docker ¶1、安装环境 deepin系统15.8 ¶2、安装过程 深度官方deepin的应用仓库已经集成了docker，不过类似docker-ce这种最新版。要想使用最新版可以参考官网 debian 安装教程安装，不过由于深度15.4基于 sid 版本开发，通过 $(lsb_release -cs) 获取的版本信息为 unstable，而docker官方源并没提供 sid 这种unstable版本的docker，所以使用官方教程是安装不成功的。 ¶（1）若之前安装过老版本，则先卸载掉之前的老版本 1$ sudo apt-get remove docker.io docker-engine ¶（2）安装docker-ce与密钥管理与下载相关工具 说明：这里主要提供curl命令、software-properties-common包提供的add-apt-repository和密钥管理工具 1$ sudo apt-get install apt-transport-https ca-cerificates curl python-software-properties software-properties-common ¶（3）下载并安装密钥 鉴于国内网络问题，强烈建议使用国内源，官方源请在注释中查看。 国内源可选用清华大学开源软件镜像站或中科大开源镜像站，示例选用了中科大的。 为了确认所下载软件包的合法性，需要添加软件源的 GPG 密钥。 123$ curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/debian/gpg | sudo apt-key add -// 官方源，能否成功可能需要看运气。 $ curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add - ¶（4）查看密钥是否安装成功 1$ sudo apt-key fingerprint 0EBFCD88 如果安装成功，会出现如下内容： 123pub 4096R/0EBFCD88 2017-02-22 Key fingerprint = 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 uid Docker Release (CE deb) &lt;docker@docker.com&gt; sub 4096R/F273FCD8 2017-02-22 ¶（5）添加docker官方仓库 然后，我们需要向 source.list 中添加 Docker CE 软件源： 123$ sudo add-apt-repository &quot;deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/debian wheezy stable&quot;//官方源$ sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/debian wheezy stable&quot; 这点很奇怪，官方在 wheezy 位置使用的是 $(lsb_release -cs)，而在deepin下执行lsb_release -cs这个命令时，而deepin显示的是unstable，而默认debian根据正式发行版本会显示是 jessie 或者wheezy 这个如果不更改成特定版本信息，在sudo apt-get update更新时就不起作用。 更正： 之所以获取的 unstable 不成功，是因为docker官方没有提供sid版本的docker。想安装必须将该部分替换成相应版本。 这里例子的debian的版本代号是wheezy，应该替换成deepin基于的debian版本对应的代号，查看版本号命令：cat /etc/debian_version，再根据版本号对应的代号替换上面命令的wheezy即可。 例如对于deepin15.5，我操作上面的命令得到debain版本是8.0，debian 8.0的代号是jessie，把上面的wheezy替换成 jessie，就可以正常安装docker,当前docker的版本为17.12.0-ce. ¶（6）更新仓库 1$ sudo apt-get update ¶（7）安装docker-ce 1$ sudo apt-get install docker-ce 在安装完后启动报错，查看docker.service的unit文件，路径为/lib/systemd/system/docker.service，把ExecStart=/usr/bin/dockerd -H fd:// 修改为ExecStart=/usr/bin/dockerd，则可以正常启动docker 启动 命令为 1$ systemctl start docker ¶（8）查看安装的版本信息 1$ docker version ¶（9）验证docker是否被正确安装且能够正常使用 1$ sudo docker run hello-world ¶3、更换国内docker加速器 若使用docker官方仓库，速度会很慢，所以更换国内加速器就显得尤为重要。 ¶（1）方法一：使用阿里云的docker加速器 1、在阿里云申请一个账号 打开链接https://cr.console.aliyun.com/#/accelerator 拷贝您的专属加速器地址 2、修改daemon配置文件/etc/docker/daemon.json俩使用加速器 12345678$ sudo mkdir -p /etc/docker$ sudo tee /etc/docker/daemon.json &lt;&lt;-&apos;EOF&apos;&#123; &quot;registry-mirrors&quot;:[您的加速器地址]&#125;EOF$ sudo systemctl daemon-reload$ sudo systemctl restart docker ¶（2）方法二：使用docker-cn提供的镜像源 1、编辑/etc/docker/daemon.json文件，并输入docker-cn镜像源地址 1$ sudo nano /etc/docker/daemon.json 输入以下内容： 123&#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]&#125; 2、重启docker服务 1$ sudo service docker restart ¶4、可视化管理 我这里是使用的Portainer作为容器的GUI管理方案 Portainer官网 安装指令： 12$ docker volume create portainer_data$ docker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer 浏览器访问localhost:9000即可进入容器管理界面： ¶5、禁止开机自启 默认情况下Docker是开机自启的，如果我们想禁用开机自启，可以通过安装chkconfig命令来管理Deepin自启项 1234# 安装chkconfig$ sudo apt-get install chkconfig# 移除自启$ sudo chkconfig --del docker]]></content>
      <categories>
        <category>应用容器</category>
      </categories>
      <tags>
        <tag>安装</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识Redis（一）之安装]]></title>
    <url>%2Fposts%2F37dcc900.html</url>
    <content type="text"><![CDATA[一、安装Redis 这里演示的是Linux系统的安装过程 官网地址：https://redis.io/ 第一步： 打开终端，在官网找到你想要的版本的链接，使用以下指令进行下载 12345$ wget http://download.redis.io/releases/redis-5.0.3.tar.gz$ tar xzf redis-5.0.3.tar.gz -C /opt (我这里将他解压到/opt目录下)$ cd /opt/redis-5.0.3$ make (因为redis是使用C语言所写的，所以需要使用make指令进行编译)$ make PREFIX=/usr/local/redis install (这里我将编译好的redis安装到/usr/local/redis目录下) 第二步：启动ｒｅｄｉｓ 12$ cd usr/local/redis (进入安装redis的目录)$ ls 这时候该redis目录下只有一个文件，是bin目录 123$ cd bin/$ lsredis-benchmark redis-check-aof redis-check-rdb redis-cli redis-sentinel redis-server 该目录下有这5个可执行文件，分别具有的功能： 12345redis-benchmark -----性能测试工具redis-check-aof -----AOF文件修复工具redis-check-dump -----RDB文件检查工具(快件持久工具)redis-cli -----命令行客户端redis-server -----redis服务器启动命令 我们使用redis-server启动redis 1$ ./redis-server 看到如下图，便安装并启动成功： 通过上图我们可以看到，redis已经启动成功，并且在6379端口监听。 我们发现了什么！这时候我们这个终端什么都不能干了，我们能不能让他在后台启动后台运行，不影响我们的后续操作呢！ 当然可以，我们去刚才make（编译）的目录下看看 看到了这个redis.conf就知道有救了，我们可以修改他的默认启动方式的配置进行后台启动 我们使用vim 指令进行编辑，找到其中的daemonize no，将他的no改为yes 这时候我们将修改后的redis.conf复制到/usr/local/redis目录下，使用： 1$ cp redis.conf /usr/local/redis 这时候我们切回到安装redis的/usr/local/redis目录下执行新的redis.conf重新启动redis 1$ ./bin/redis-server ./redis.conf 咦咦咦！！！这是什么情况！出现意外了！！！ 原来我们刚才运行的redis已经将端口占用了，我们去查询一下现在运行的相关redis进程都有哪些 1$ ps -ef | grep -i redis 我们要把这个正在运行的redis结束掉，有两种方法 kill -9 6894 ./bin redis-server shutdown 结束redis进程后我们重新启动，继续使用刚才修改的配置redis-conf进行启动 1$ ./bin/redis-server ./redis.conf 结果如下： 第三步：打开命令行客户端redis-cli进行简单的存储操作 1$ ./bin/redis-cli 通过下图的简单指令你可以简单的进行一些增删查操作： 二、为Python安装Redis客户端 123$ wget -q http://peak.telecommunity.com/dist/ez_setup.py$ sudo python ez_setup.py$ sudo python -m easy_install redis hiredis 发现没有任何报错后进行测试 1234567$ python$ import redis$ conn = redis.Redis()$ conn.set('name','Hello!Redis')True$ conn.get('name')Hello!Redis 若与我上面的指令一样，而且未出现其他错误，即可认为自己的redis客户端安装成功]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>NoSql</tag>
        <tag>Redis</tag>
        <tag>安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python入门]]></title>
    <url>%2Fposts%2Fd9ef328.html</url>
    <content type="text"><![CDATA[python的语言格式不严格 弱数据类型语言，根据你的赋值情况来确定变量的类型 12345a = 10a = &apos;还可以赋值字符串&apos;int b = 10# b = &apos;报错&apos; 若使用具体的数据类型进行定义，则不能进行其他类型的数据进行更改数据 Python的注释是使用#进行注释 ¶1、print print (‘这样可以输出一句话’) print(‘This is’,‘Python’) 逗号会被更换为空格 ¶2、变量 在Python程序中，变量是用一个变量名表示，变量名必须是大小写英文、数字和下划线（_）的组合，且不能用数字开头， a = 1 变量a是一个整数。 t_007 = ‘T007’ 变量t_007是一个字符串 执行a = ‘ABC’，解释器创建了字符串 'ABC’和变量 a，并把a指向 ‘ABC’： 执行b = a，解释器创建了变量 b，并把b指向 a 指向的字符串’ABC’： 执行a = ‘XYZ’，解释器创建了字符串’XYZ’，并把a的指向改为’XYZ’，但b并没有更改： 所以，最后打印变量b的结果自然是’ABC’了 ¶3、字符串的定义 若字符串本身包括&quot;,那么就使用单引号包起来，反之亦然 &quot;I'm OK&quot; 'Learn &quot;Python&quot; is very interesting' 若既包含’和&quot;,在前面加\ 'Bob said \&quot;I\'m OK\&quot;.' 若需要转义的字符比较多，那么应该在其前面加r，就不需要一个一个转义了 r'\(~_~)/ \(~_~)/' 多行字符串 '''Line 1 Line 2 Line 3''' 若多行字符串并且需要转义字符较多，可以在其前加r’’’ r'''Python is created by &quot;Guido&quot;. It is free and easy to learn. Learn &quot;Python&quot; is very interesting!''' 四则运算 按照正常的数学四则运算进行计算 11/4 ==&gt; 2 11.0/4 ==&gt; 2.75 11%4 ==&gt;3 布尔类型 与运算：只有两个布尔值都为 True 时，计算结果才为 True。 True and True # ==&gt; True True and False # ==&gt; False False and True # ==&gt; False False and False # ==&gt; False 或运算：只要有一个布尔值为 True，计算结果就是 True。 True or True # ==&gt; True True or False # ==&gt; True False or True # ==&gt; True False or False # ==&gt; False 非运算：把True变为False，或者把False变为True： not True # ==&gt; False not False # ==&gt; True 在Python中，布尔类型还可以与其他数据类型做 and、or和not运算，请看下面的代码： a = True print a and 'a=T' or 'a=F' 计算结果不是布尔类型，而是字符串 ‘a=T’，这是为什么呢？ 因为Python把0、空字符串’'和None看成 False，其他数值和非空字符串都看成 True，所以： True and ‘a=T’ 计算结果是 'a=T’ 继续计算 ‘a=T’ or ‘a=F’ 计算结果还是 'a=T’ 要解释上述结果，又涉及到 and 和 or 运算的一条重要法则：短路计算。 1. 在计算 a and b 时，如果 a 是 False，则根据与运算法则，整个结果必定为 False，因此返回 a；如果 a 是 True，则整个计算结果必定取决与 b，因此返回 b。 2. 在计算 a or b 时，如果 a 是 True，则根据或运算法则，整个计算结果必定为 True，因此返回 a；如果 a 是 False，则整个计算结果必定取决于 b，因此返回 b。 所以Python解释器在做布尔运算时，只要能提前确定计算结果，它就不会往后算了，直接返回结果。 ¶4、if语句 因为python使靠缩进来保证代码的区域作用范围的，因此在if中我们需要严格遵守缩进规则，if中的代码需要比if多空四格（不用需使用Tab，更不能混用） score = 75 if score &gt;= 60: print('passed') else: print('failed') 多层次，多条件的选择，就像C/Java中的if…else…else一样，Python中也存在 if ???: print('满足if条件') elif ???: print('满足这个else if') else: print('满足这个else条件') ¶5、for循环 如果一次要将一个数组中的所有东西进行输出，在C/Java中 for(int i = 0;i &lt; a.length();i++){ printf(&quot;%d&quot;,a[i]); } Java代码省略 那么在Python中的For语句是如何的呢 for s in a: print(s) 这个s是新声明的变量，代表a数组中的一个个元素，不断的更新他的值，然后进行输出 ¶6、List(有序) list是一种有序的集合 L = ['Tom',10.0,'Jack',25.3,'Jim',68.2] print L &gt;&gt;&gt;['Tom',10.0,'Jack',25.3,'Jim',68.2] 按照索引访问list L[0]第一个位置的内容 L[-1]倒数第一位置的内容 添加新元素 L.append('Paul') 将Paul加到L的最后面 L.insert(2,'Paul')将Paul添加到索引为2的位置后面的以此向后推 删除一个元素 L.pop() 删除最后一个 L.pop(1) 删除第二个 ¶7、tuple不可变数组 该数组一旦创建，无法修改，只可以访问 T = (0,1,2,3,4,5,6,7,8,9) T = (1) ==&gt;1 这时候你会发现这个被解释成了1而不是一个数组 为了解决歧义问题，当这个数组只有一个元素的时候末尾使用一个逗号，例如 T = (1,) tuple的不可变指的是其元素指向的地址不变，假若使 T = (‘a’,‘b’,[‘A’,‘B’]),则AB是可以变得，因为T中第三个元素指的是list这个整体，只要是同一个list，里面的内容无论如何变，都不违背其不可变性 ¶8、dict(无序) 这是以一种key==&gt;value形式存在的集合，他的声明像json格式 d = { 'Adam': 95, 'Lisa': 85, 'Bart': 59 } 使用len(d)可以计算任意集合的大小 访问dict 对于dict的访问，可以使用d[key]进行访问，也可以使用d.get(key)进行访问 &gt;&gt;&gt; print(d['Adam']) &gt;&gt;&gt; print(d.get('Adam')) 以上两种方法可以访问到key为Adam的值输出均为： 95 以上两种访问方法，尽量使用d.get[key]的方式 使用d[key]访问，若该key不存在，则会报出KeyError的错误 使用d.get(key)，则会输出None 更新dict 想要将dict中的一些值进行更新或添加新元素，那么直接使用访问的方法进行访问然后进行=操作进行赋值 d['Adam'] = 99 d['Tom'] = 100 //这里的Tom原本不存在 注：这里不能使用d.get(key)的进行赋值 遍历dict dict的遍历，使用for并且遍历只是对key的遍历，例： for name in d: print(name,':',d.get(name)) 输出结果： Lisa : 85 Adam : 95 Bart : 59 ¶9、set set持有一些列元素，并且无重复、无序、不变 创建方式 &gt;&gt;&gt; s = set(['a','c','s']) 这里的创建就是向其中传入List，list的元素将作为set的元素 判断是否在该set中 &gt;&gt;&gt; 'a' in s True &gt;&gt;&gt; 'r' in s False 这里区分大小写，若其中一个字母不符合，那么就会进行误判 若想要解决其中的大小写问题，那么就应该在判断的时候忽略大小写 忽略大小写的判断 使用capitalize()方法进行忽略大小写 &gt;&gt;&gt; 'A'.capitalize() True 这里可以忽略大小写的不同进行判断 set中存list并进行取出 s = set([('Adam', 95), ('Lisa', 85), ('Bart', 59)]) for x in s: L = list(x) print L[0],':',L[1] set操作 现在有 s = set([1,2,3]) 增 添加元素时，使用add()方法进行添加 s.add(4) //===&gt;set[1,2,3,4] s.add(3) //===&gt;set[1,2,3] 若存在重复的元素，则不会被加入到其中 删 删除元素时，使用remove()方法 s.remove(4) //报错，不存在4这个元素 Demo: 针对下面的set，给定一个list，对list中的每一个元素，如果在set中，就将其删除，如果不在set中，就添加进去。 s = set(['Adam', 'Lisa', 'Paul']) L = ['Adam', 'Lisa', 'Bart', 'Paul'] for l in L: if l in s: s.remove(l) else: s.add(l) print s ¶10、函数 python中存在很多函数，接下来会介绍很多简单的函数 ¶1、abs(x) 取绝对值的函数 &gt;&gt;&gt; abs(-20) 20 &gt;&gt;&gt; abs(12.34) 12.34 ¶2、cmp(x,y) 比较两个参数大小的函数，如果 x&lt;y，返回 -1，如果 x==y,返回 0;如果 x&gt;y，返回 1： &gt;&gt;&gt; cmp(1, 2) -1 &gt;&gt;&gt; cmp(2, 1) 1 &gt;&gt;&gt; cmp(3, 3) 0 ¶3、int(x) 将传入的参数进行整型化 &gt;&gt;&gt; int('123') 123 &gt;&gt;&gt; int(12.34) 12 ¶4、str(x) 将传入的参数字符串化 &gt;&gt;&gt; str(123) '123' &gt;&gt;&gt; str(1.23) '1.23' ¶5、sum(x) 传入的数据进行求和，传入一个list，会将所有的元素进行求和 例：sum()函数接受一个list作为参数，并返回list所有元素之和。请计算 11 + 22 + 33 + … + 100100。 L = [] n = 0 while n &lt; 100: n += 1 L.append(n*n) print sum(L) ¶11、编写函数 定义一个函数需要使用def开头，例如： 一个求平方和的函数： def square_of_sum(L): L1 = [] for l in L: L1.append(l*l) return sum(L1) 1、Python中没有+=、-=这些 2、Python中没有三元运算符，需要用相应的if…else进行替代 ¶12、复杂表达式 利用py的字符串拼接也可以生成一个前端的简单东西 这里使用table作为案例： d = { 'Adam': 95, 'Lisa': 85, 'Bart': 59 } def generate_tr(name, score): if score &lt; 60: return '&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td style=&quot;color:red&quot;&gt;%s&lt;/td&gt;&lt;/tr&gt;' % (name, score) return '&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;' % (name, score) tds = [generate_tr(name,score) for name, score in d.iteritems()] print '&lt;table border=&quot;1&quot;&gt;' print '&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Score&lt;/th&gt;&lt;tr&gt;' print '\n'.join(tds) print '&lt;/table&gt;' 运行结果： ¶13、条件过滤 列表生成式的for循环后面可以加上if判断： 原代码： &gt;&gt;&gt; [x * x for x in range(1, 11)] [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 若我们只想要偶数的平方，可以在if后面加上条件判断： 更改后的代码： &gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0] [4, 16, 36, 64, 100] 有了 if 条件，只有 if 判断为 True 的时候，才把循环的当前元素添加到列表中。 ¶14、多层表达式 可以用多层表达式进行多层组装： 对于字符串 ‘ABC’ 和 ‘123’，可以使用两层循环，生成全排列： &gt;&gt;&gt; [m + n for m in 'ABC' for n in '123'] ['A1', 'A2', 'A3', 'B1', 'B2', 'B3', 'C1', 'C2', 'C3'] 利用 3 层for循环的列表生成式，找出对称的 3 位数。例如，121 就是对称数，因为从右到左倒过来还是 121： print [100 * n1 + 10 * n2 + n3 for n1 in range(1,10) for n2 in range(10) for n3 in range(10) if n1 == n3]]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[四类NoSql数据库]]></title>
    <url>%2Fposts%2F666351ef.html</url>
    <content type="text"><![CDATA[NoSQL的特点 易扩展 大数据量，高性能 灵活的数据类型 高可用 ¶一、键值类（key-value） ¶1、相关产品 Tokyo Cabinet/Tyrant Redis（最常用） Voldemort Berkeley DB ¶2、应用场景 内容缓存，主要用户处理大量数据的高访问负载 ¶3、数据模型 一系列键值对 ¶4.优点 快速查询 ¶5、缺点 存储的数据缺少结构化 ¶二、列存储数据库 ¶1、相关产品 Cassandra HBase Riak ¶2、应用场景 分布式的文件系统 ¶3、数据模型 以列簇式存储，将同一列数据存在一起 ¶4.优点 查询速度快，可扩展性强，更容易进行分布式扩展 ¶5、缺点 功能相对局限 ¶三、文档型数据库 ¶1、相关产品 CouchDB MongoDb ¶2、应用场景 Web应用（与key-value类似，value是结构化的） ¶3、数据模型 一系列键值对 ¶4、优点 数据结构要求不严格 ¶5、缺点 查询性能不高，而且缺乏统一的查询语法 ¶4、图形（Graph）数据库 ¶1、相关产品 Neo4j IofoGrid Infinite Graph ¶2、应用场景 社交网络，推荐系统等，专注于构建关系图 ¶3.数据模型 图结构 ¶4、优点 利用图结构相关算法 ¶5、缺点 需要对整个图做计算才能得出结果，不容易做分布式的集群方案 ¶备注：以上是对四类NoSQL的简单介绍与区别，后期会对各类中典型的数据库进行介绍分析，有待完善]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>NoSql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Git Commit]]></title>
    <url>%2Fposts%2F9c4f0ba6.html</url>
    <content type="text"><![CDATA[git commit 的重要性 当你学会使用git，并在GitHub上建立了自己的Repositories时。 嗯。可以push自己的代码了，一顿 git pull origin master git add . git commit -m&quot;balabala&quot; git push origin master 看到100%之后开心极了。几个月后看到如下 请问上传的注册页面在哪里呢？？？想必你也心里充满了疑虑，我到底放在哪个里面的？？ 由此可见git add和git commit并不是很简单的一次全部完成的 正确的使用git add 和 git commit 当你每次push的时候，一定是更新了一些代码，完善了一些功能。 例如： 1. 注册功能 2. 登录功能 3. 完善了README 那么该如何的push这次代码呢？？ 提交注册功能的功能代码 12git add src/registergit commit -m&quot;add register function&quot; 提交登录功能的代码 12git add src/logingit commit -m&quot;add login function&quot; 提交完善的README 12git add READM.mdgit commit -m&quot;modify README&quot; 做完以上之后，你就可以正常的 git push origin mater 当然，这样只是一个简单push 进一步的规范你的git commit 你以为做到上面这些你就可以完成规范的git commit 了吗？？ 太天真你，一般规范的commit需要由三部分构成 &lt;type&gt;(&lt;scope&gt;): &lt;subject&gt; // 空一行 &lt;body&gt; // 空一行 &lt;footer&gt; 其中，Header 是必需的，Body 和 Footer 可以省略。 不管是哪一个部分，任何一行都不得超过72个字符（或100个字符）。这是为了避免自动换行影响美观。 ¶1.Header Head这部分只有一行，但是包括三个部分 &lt; type &gt;(必需)&lt; scope &gt;（可选）&lt; subject &gt;（必需） ¶(1) &lt; type &gt; type用来说明commit的类别，也就是说别人看了你的type就知道你这次push的性质是什么，只允许有以下几种标识 init: 初始化项目,往往用于仓库刚刚建立，建好项目框架之后的一次push feat: 新功能(feature) docs: 文档的提交(document) fix: 修补bug style: 格式的改动(不影响代码运行的变动，往往是规范了代码的格式) refactor: 重构(既不增加新功能，也不改任何的bug) test: 增加测试 chore: 构建过程或辅助工具的变动 opt: 优化和改善，比如弹窗进行确认提示等相关的，不会改动逻辑和具体功能等 other: 用于难以分类的类别（不建议使用，但一些如删除不必要的文件，更新.ignore之类的可以使用） 如果type为feat和fix，则该 commit 将肯定出现在 Change log 之中。其他情况（docs、chore、style、refactor、test）由你决定，要不要放入 Change log，建议是不要。 ¶(2) &lt; scope &gt; scope用于说明commit的影响范围，比如数据层，控制层，视图层等 （可选）类型后面可以加上括号，括号内填写主要变动的范围，比如按功能模块分，某模块；或按项目三层架构模式分，分数据 层、控制层之类的。 #：表示模块 #student --&gt; 表示 学生模块 （具体的模块开头字母小写，驼峰命名） #ALL --&gt; 表示 所有模块 （特殊含义如ALL表所有，MOST表大部分，用大写字母表示） #MOST --&gt; 表示 大部分模块 e.g. feat(#student): 新增添加学生的功能 —— 表示student模块新增功能，功能是添加学生 ¶(3)&lt; subject &gt; subject是 commit 目的的简短描述，不超过50个字符。 - 以动词开头，使用第一人称现在时，比如change，而不是changed或changes - 第一个字母小写 - 结尾不加句号（.） ¶2. Body body部分是对本次commit的详细描述，可以分成多行进行描述可以分成多行，正文在 72 个字符处换行。 使用正文解释是什么(what)和为什么(why)，而不是如何做，以及与以前行为的对比。 于是可以这样写： balabala : balabala what: balabala why: balabala ¶3. Footer Footer只用于两种情况 ¶(1)不兼容变动 如果当前代码与上一个版本不兼容，则 Footer 部分以BREAKING CHANGE开头，后面是对变动的描述、以及变动理由和迁移方法。 BREAKING CHANGE: isolate scope bindings definition has changed. To migrate the code follow the example below: Before: scope: { myAttr: 'attribute', } After: scope: { myAttr: '@', } The removed `inject` wasn't generaly useful for directives so there should be no code using it. ¶（2）关闭 Issue 如果当前 commit 针对某个issue，那么可以在 Footer 部分关闭这个 issue 。 Closes #234 也可以一次关闭多个 issue 。 Closes #123, #245, #992 ¶4. Revert 还有一种特殊情况，如果当前 commit 用于撤销以前的 commit，则必须以revert:开头，后面跟着被撤销 Commit 的 Header。 revert: feat(pencil): add 'graphiteWidth' option This reverts commit 667ecc1654a317a13331b17617d973392f415f02. Body部分的格式是固定的，必须写成This reverts commit &lt;hash&gt;.，其中的hash是被撤销 commit 的 SHA 标识符。 如果当前 commit 与被撤销的 commit，在同一个发布（release）里面，那么它们都不会出现在 Change log 里面。如果两者在不同的发布，那么当前 commit，会出现在 Change log 的Reverts小标题下面。 几个优秀的撰写合格的commit的工具 ¶1. Commitizen Commitizen是一个撰写合格 Commit message 的工具。 安装命令如下。 $ npm install -g commitizen 然后，在项目目录里，运行下面的命令，使其支持 Angular 的 Commit message 格式。 $ commitizen init cz-conventional-changelog --save --save-exact 以后，凡是用到git commit命令，一律改为使用git cz。这时，就会出现选项，用来生成符合格式的 Commit message。 ¶2. validate-commit-msg validate-commit-msg 用于检查 Node 项目的 Commit message 是否符合格式。 它的安装是手动的。首先，拷贝下面这个JS文件，放入你的代码库。文件名可以取为validate-commit-msg.js。 接着，把这个脚本加入 Git 的 hook。下面是在package.json里面使用 ghooks，把这个脚本加为commit-msg时运行. &quot;config&quot;: { &quot;ghooks&quot;: { &quot;commit-msg&quot;: &quot;./validate-commit-msg.js&quot; } } 然后，每次git commit的时候，这个脚本就会自动检查 Commit message 是否合格。如果不合格，就会报错。 $ git add -A $ git commit -m &quot;edit markdown&quot; INVALID COMMIT MSG: does not match &quot;&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;&quot; ! was: edit markdown ¶3. conventional-changelog 如果你的所有 Commit 都符合 Angular 格式，那么发布新版本时， Change log 就可以用脚本自动生成。 生成的文档包括以下三个部分。 - New features - Bug fixes - Breaking changes. 每个部分都会罗列相关的 commit ，并且有指向这些commit的链接。当然，生成的文档允许手动修改，所以发布前，你还可以添加其他内容。 conventional-changelog 就是生成 Change log 的工具，运行下面的命令即可。 $ npm install -g conventional-changelog $ cd my-project $ conventional-changelog -p angular -i CHANGELOG.md -w 上面命令不会覆盖以前的 Change log，只会在CHANGELOG.md的头部加上自从上次发布以来的变动。 如果你想生成所有发布的 Change log，要改为运行下面的命令。 $ conventional-changelog -p angular -i CHANGELOG.md -w -r 0 为了方便使用，可以将其写入package.json的scripts字段。 { &quot;scripts&quot;: { &quot;changelog&quot;: &quot;conventional-changelog -p angular -i CHANGELOG.md -w -r 0&quot; } } 以后，直接运行下面的命令即可。 $ npm run changelog (完) ¶以上内容仅代表作者自己的个人观点，欢迎广大专业人士提出建议 Thank You！！]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String、StringBuilder和StringBuffer]]></title>
    <url>%2Fposts%2F2871eb40.html</url>
    <content type="text"><![CDATA[运行速度 StringBuilder &gt; StringBuffer &gt; String ¶String运行速度慢的原因 数据类型的不同 1）String为字符串常量 字符串常量存储于常量池中，不可以被更改 123String str = &quot;i am&quot;;str = str + &quot;Tom&quot;;这种并不是直接在原有的str上增加了&quot;Tom&quot;,而是JVM回收了之前的str，产生新的str，并将&quot;i am Tom&quot;赋值给了它 2)StringBuilder和StringBuffer为字符串变量 字符串变量存放在栈中，从栈中读取数据的速度仅次于寄存器 123456与上面同样的操作StringBuffer str = new StringBuffer();StringBuffer str1 = mew StringBuffer(&quot;i am&quot;);StringBuffer str2 = new StringBuffer(&quot;Tom&quot;);str = str1 + str2;//此运行速度不是最快str = str.append(str1).append(str2);//使用Java中自带的函数，运行速度会加快，可以使语句在运行时，减少创建对象的数量，从而减少运行时间 ¶线程安全 在线程安全上，StringBuilder是线程不安全的，而StringBuffer是线程安全的 如果一个StringBuffer对象在字符串缓冲区被多个线程使用时，StringBuffer中很多方法可以带有synchronized关键字，所以可以保证线程是安全的，但StringBuilder的方法则没有该关键字，所以不能保证线程安全，有可能会出现一些错误的操作。 所以如果要进行的操作是==多线程==的，那么就要使用==StringBuffer==，但是在==单线程==的情况下，还是建议使用速度比较快的==StringBuilder==。 ¶小结 ¶String：适用于少量的字符串操作的情况 ¶StringBuilder：适用于单线程下在字符缓冲区进行大量操作的情况 ¶StringBuffer：适用多线程下在字符缓冲区进行大量操作的情况]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单工厂模式]]></title>
    <url>%2Fposts%2F6b3bea20.html</url>
    <content type="text"><![CDATA[¶level 1 拿到一个计算器的题目，大家一定会想到，使用switch判断符号，进入分支后计算 demo: 1234567891011121314151617181920212223242526272829Operation.java public static Operation getResult(double numberA,double numberB,String operate)&#123; double result = 0; //判断类型，创建对象 switch (operate)&#123; case &quot;+&quot;: result = numberA + numberB; break; case &quot;-&quot;: result = numberA - numberB; break; case &quot;*&quot;: result = numberA * numberB; break; case &quot;/&quot;: result = numberA / numberB; break; default: System.out.println(&quot;输入有误！请检查后重新输入!&quot;); &#125; return result; &#125; Client.java public static void main(String[] args)&#123; //此处略去输入赋值 double result = Operation.getResult(numberA,numberB,operate); System.out.println(&quot;计算结果为:&quot;+result); &#125; 这样一个简单的计算器就行写好了，但是你有没有发现一个问题，虽然将服务端和客户端进行了分离，但是如果需要完善这个计算器，增加新的运算方式，比如开平方的sqrt运算？？？如何解决？？ 解决方案： ​ ​ 更改Operation.java文件，在switch中增加一个分支。 结果： 1、这样仅仅是增加一个新的算法，却要使所有的算法都重新进行编译，也就是一个算法可以影响到所有的算法，这样使代码的 耦合度太高，不利于再次开发与维护。 2、另一方面就是每次增加功能点，你需要在集合了所有的算法的类中进行修改，一个失误操作可以让整个项目陷入泥潭 ¶level 2 我们在level 1中的方案已经发现了他的利弊，那么如何来让自己的项目更加完善，使代码的耦合度降低，增大代码的可用性，可伸展性 我们可以试一试继承： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556Operation.java package designpatterns.simplefactorymode.service; /** * @author :DengSiYuan * @date :2018/12/2 10:47 * @desc : 该类是对数字的封装以及运算结果的处理 */ public class Operation &#123; private double numberA = 0; private double numberB = 0; public double getNumberA() &#123; return numberA; &#125; public void setNumberA(double numberA) &#123; this.numberA = numberA; &#125; public double getNumberB() &#123; return numberB; &#125; public void setNumberB(double numberB) &#123; this.numberB = numberB; &#125; //返回结果值的方法 public double getResult() throws Exception &#123; double result = 0; return result; &#125;&#125;OperationAdd.java package designpatterns.simplefactorymode.service;/** * @author :DengSiYuan * @date :2018/12/2 11:37 * @desc :加法运算 */public class OperationAdd extends Operation &#123; @Override public double getResult()&#123; double result = 0; result = getNumberA() + getNumberB(); return result; &#125;&#125;剩下的乘除亦然 这样使用继承，我们将所有运算共同需要的属性：两个数字和一个方法：算出结果，放入到他们共同的父类中，这样每次需要增加新的运算方式，只需要去继承这个Operation，如有其他特殊要求，进行扩展即可。 那么问题又来了：我们每次如何去判断这次是使用的什么运算方式（+？-？*？/），然后去创建相应的对象，这样我们就需要一个&quot;工厂&quot;来管理，举个例子： 我们去工厂告诉工厂主管说我们需要一个IPhone X,这时候他们知道了你所需要的手机类型后，告诉下面的工程师，创建了一个实体IPhone X对象。 程序中的工厂和现实中的一样，我们需要在程序的入口给工厂一个Type，他会根据你所告诉他的Type，创建一个你所需要使用的匹配的对象实体，调用使用他的方法 接下来我们来看Level 3 ¶level 3 到了我们的重点：简单工厂模式 先看一下他的UML图吧 这是一个简单的设计 start —&gt; OperationFactory —&gt; 判断类型，动态创建对象 —&gt; 确切的知道那个类，使用其方法 Operation.java(所有运算的父类) 12345678910111213141516171819202122232425262728293031323334package designpatterns.simplefactorymode.service;/** * @author :DengSiYuan * @date :2018/12/2 10:47 * @desc : 该类是对数字的封装以及运算结果的处理 */public class Operation &#123; private double numberA = 0; private double numberB = 0; public double getNumberA() &#123; return numberA; &#125; public void setNumberA(double numberA) &#123; this.numberA = numberA; &#125; public double getNumberB() &#123; return numberB; &#125; public void setNumberB(double numberB) &#123; this.numberB = numberB; &#125; //返回结果值的方法 public double getResult() throws Exception &#123; double result = 0; return result; &#125;&#125; OperationFactory.java(管理对象的生成的工厂类) 12345678910111213141516171819202122232425262728293031package designpatterns.simplefactorymode.service;/** * @author :DengSiYuan * @date :2018/12/2 12:19 * @desc :工厂模式，程序的入口，动态创建符合类型的对象 */public class OperationFactory &#123; public static Operation createOperation(String type)&#123; Operation operation = null; //判断类型，创建对象 switch (type)&#123; case &quot;+&quot;: operation = new OperationAdd(); break; case &quot;-&quot;: operation = new OperationSub(); break; case &quot;*&quot;: operation = new OperationMul(); break; case &quot;/&quot;: operation = new OperationDiv(); break; default: System.out.println(&quot;输入有误！请检查后重新输入!&quot;); &#125; return operation; &#125;&#125; OperationAdd.java(加法类) 1234567891011121314151617package designpatterns.simplefactorymode.service;/** * @author :DengSiYuan * @date :2018/12/2 11:37 * @desc :加法运算 */public class OperationAdd extends Operation &#123; @Override public double getResult()&#123; double result = 0; result = getNumberA() + getNumberB(); return result; &#125;&#125; Client.java(客户端)大家可以去实现更加通用的，用户输入运算式，判断符号（封装成一个方法） 1234567891011121314151617181920212223package designpatterns.simplefactorymode.client;import designpatterns.simplefactorymode.InputDeal;import designpatterns.simplefactorymode.service.Operation;import designpatterns.simplefactorymode.service.OperationFactory;import java.util.Scanner;/** * @author :DengSiYuan * @date :2018/12/2 12:27 * @desc :用户端 */public class UserClient &#123; public static void main(String[] args) throws Exception &#123; String[] test = &#123;&quot;5&quot;,&quot;+&quot;,&quot;8&quot;&#125;; String input = InputDeal.getEveryValue(test); Operation operation = OperationFactory.createOperation(input); operation.setNumberA(Integer.parseInt(test[0])); operation.setNumberB(Integer.parseInt(test[2])); System.out.println(operation.getResult()); &#125;&#125; 大家可以看到一个初级的使用简单工厂模式的计算器，根据一步一步的level比较，可以清楚的看到简单工厂模式存在的必要性以及他的优点。 持续更新设计模式！！]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>简单工厂模式</tag>
      </tags>
  </entry>
</search>
